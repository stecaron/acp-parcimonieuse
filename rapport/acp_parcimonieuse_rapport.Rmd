---
title: "ACP Parcimonieuse"
author: "St√©phane Caron/Sofia Harrouch"
date: '2018-03-13'
bibliography: bibliography.bib
output: 
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Introduction

Les m√©thodes statistiques de r√©duction de la dimensionnalit√© ont g√©n√©ralement comme objectif de r√©duire la dimension d'un jeu de donn√©es dans le but de simplifier l'interpr√©tation des donn√©es, de permettre la visualisation des donnn√©es ou m√™me de d'am√©liorer la performance de certaines m√©thodes appliqu√©es sur ces donn√©es r√©duites. En termes simples, r√©duire la dimensionnalit√© revient √† r√©duire le nombre de variables (p) mesur√©es.

L'analyse en composante principale est une m√©thode classique de r√©duction de la dimensionnalit√©. Cette m√©thode permet de cr√©er des combinaisons lin√©aires des diff√©rentes variables du jeu de donn√©es tout en conservant le plus de variabilit√© possible. Chacune des nouvelles composantes principales cr√©es poss√®dent un vecteur de coefficients de saturation (loadings) de dimension $p\text{ x }1$, correspondant en quelque sorte √† l'importance attribu√©e √† chacune des diff√©rentes variables originales du jeu de donn√©es. Il est donc possible d'interpr√©ter ces coefficients de saturation et d'obtenir une interpr√©tation plus g√©n√©ralis√©e de certaines composantes principales calcul√©es.

Cependant, cette interpr√©tation peut se r√©v√®ler assez complexe dans le cas o√π une composante principale est expliqu√©e (coefficients de saturation √©lev√©s) par plusieurs variables originales du jeu de donn√©es. De plus, il peut √™tre difficile de d√©finir √† partir de quelle valeur exactement un coefficient de saturation est consid√©r√© comme √©tant "non important" pour une composante principale. Pour palier √† ce probl√®me d'interpr√©tation, il existe diff√©rentes diff√©rentes m√©thodes connues. Par exemple, les rotations [@Jolliffe1989] cherche √† simplifier l'interpr√©tation des composantes principales. Il pourrait √©galement √™tre possible d'√©carter les coefficients de saturation inf√©rieurs √† une certaine valeur ou simplement restreindre les valeurs possibles que ces coefficients peuvent prendre (ex: -1, 0 ou 1). Ces m√©thodes sont des exemples de strat√©gie permetttant de faciliter l'interpr√©tation des composantes principales, mais elles ont tous certains d√©savantages.

La m√©thodologie introduite dans le pr√©sent document [@Jolliffe2003] est en quelque sorte une alternative √† ces m√©thodes. En bref, elle consiste √† ajouter certaines contraintes au mod√®le d'analyse en composante principale qui auront comme objectif d'am√©liorer l'interpr√©tabilit√© des composantes d√©riv√©es. Cela permettra notamment d'obtenir des coefficients de saturation exactement √©gale √† zero. On pourrait donc dire que cette m√©thode permet de combiner l'aspect r√©duction de la dimensionnalit√© apport√©e par l'ACP et l'aspect simplification de l'interpr√©tabilit√© apport√© par les exemples d√©crits plus haut. 

La section 2 fera l'illustration du genre de probl√®me qu'on peut √©prouver avec l'analyse en composante principale et les rotations en terme d'interpr√©tatibilit√©. La section 3 a comme objectif de d√©crire la m√©thodologie. Dans la section 4, nous verrons plus en d√©tails la justification th√©orique et les r√©sultats de simulation de la m√©thodologie. La section 5 permettra d'illustrer avec un exemple complet les r√©sultats de la m√©thodologie. Finalement, la section 6 aura comme but de conclure bri√®vement en plus de mentionner d'autres √©l√©ments √† savoir √† propos de la m√©thodologie.

## Exemple de motivation

Pour illustrer la motivation derr√®re la m√©thodologie, supposons qu'on cherche √† simplifier un jeu de donn√©es provenant d'un √©chantillon de 180 coupes de bois de pin afin d'avoir une meilleure comp√©hension des diff√©rentes mesures (variables) impliqu√©es. Les diff√©rentes variables du jeu de donn√©es en question sont pr√©sent√©es dans le tableau 1. √Ä partir de la matrice de corr√©lation, il est possible de commencer par faire l'analyse en composante principale et analyser les diff√©rentes composantes calcul√©es.

```{r dataset}
library(elasticnet)
library(data.table)
data(pitprops)
data_description <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops)))),
  description = c("Diam√®tre dans le haut de l'arbre (en pouces)", "Longeur (en pouces)", "Humidit√© (% poids sec)", "Gravit√© au moment du test", "Nombre d'anneaux dans le haut de l'arbre", "Nombre d'anneaux dans le haut de l'arbre", "Branche principale (en pouces)", "Distance du bout de la branche principale au haut de l'arbre", "Nombre de spires", "Longueur de l'h√©lice transparente du haut (en pouces)", "Nombre moyen de n≈ìuds par verticille", "Diam√®tre moyen des noeuds (en pouces)")
)
knitr::kable(x = data_description, col.names = c("Variables", "Description"), row.names = FALSE, align = c("c", "l"))
```

Comme mentionn√© dans l'introduction, l'ACP consiste essentiellement √† trouver des combinaisons lin√©aires des variables originales du jeu donn√©es (disons la matrice $X$) tout en maximisant la variance. En termes plus th√©oriques, la premi√®re composante principale est calcul√©e en maximisant la fonction

$$
F(\alpha_1) = \alpha_1' \Sigma \alpha_1
$$
avec la contrainte que $\alpha_1'\alpha_1=1$. La matrice $\Sigma$ correspond √† la matrice de covariance ou √† la matrice de corr√©lation (d√©pend situation).

Le vecteur $\alpha_1$ correspond au vecteur de coefficients de saturation de la premi√®re composante principale. On refait la m√™me chose pour la deuxi√®me composante principale en ajoutant la contrainte que

$$
cov(\alpha_1'X, \alpha_2'X) = 0
$$
Il est √©galement possible de d√©montrer que si on fait la d√©composition en valeurs singuli√®res de la matrice de corr√©lation (ou covariance), on trouve que les vecteurs $\alpha1, ..., \alpha_p$ correspondent aux vecteurs propres de la matrice $\Sigma$ alors que la variance de chacune des composantes principales correspond aux $p$ valeurs propres de la m√™me matrice $\Sigma$. 

```{r acp}
options(scipen = 999)
acp <- prcomp(x = pitprops, center = FALSE)

data_pca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Simplicity", "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(acp$rotation[, 1]), 0, acp$sdev[1]/sum(acp$sdev), cumsum(acp$sdev)[1]/sum(acp$sdev)), 3),
  PC2 = round(c(as.numeric(acp$rotation[, 2]), 0, acp$sdev[2]/sum(acp$sdev), cumsum(acp$sdev)[2]/sum(acp$sdev)), 3),
  PC3 = round(c(as.numeric(acp$rotation[, 3]), 0, acp$sdev[3]/sum(acp$sdev), cumsum(acp$sdev)[3]/sum(acp$sdev)), 3),
  PC4 = round(c(as.numeric(acp$rotation[, 4]), 0, acp$sdev[4]/sum(acp$sdev), cumsum(acp$sdev)[4]/sum(acp$sdev)), 3),
  PC5 = round(c(as.numeric(acp$rotation[, 5]), 0, acp$sdev[5]/sum(acp$sdev), cumsum(acp$sdev)[5]/sum(acp$sdev)), 3),
  PC6 = round(c(as.numeric(acp$rotation[, 6]), 0, acp$sdev[6]/sum(acp$sdev), cumsum(acp$sdev)[6]/sum(acp$sdev)), 3)
)
knitr::kable(data_pca, row.names = FALSE, col.names = c("", paste0("PC", seq(1:6))), align = rep("c", 6))
```


## Description de la m√©thodologie
Comme mentionnÈ prÈcÈdemment, L'ACP souffre parfois du problËme d'interprÈtation des axes. Pour cela on a recours ‡ une nouvelle mÈthode **ACP parcimonieuse (SPARSE)** qui nous donne des axes "Sparses" expliquÈs par un petit nombre des variables; une mÈthode qui ignore l'effet de certaines variables sur les axes principales.
Dans cette section, on expliquera les diffÈrentes mÈthodes proposÈes pour l'estimation de ces axes. La premiËre mÈthode est basÈe sur la propriÈtÈ d'obtention d'une variance maximale des composantes principale(SCoTLASS), la deuxiËme mÈthode est construit en se basnt sur la propriÈtÈ de l'erreur de recontruction/ de regression( SPCA) et la derniËre mÈthode est celle obtenue en utilisant la dÈcomposition PMD (PCA).

### La premiËre mÈthode: SCoTLASS
Une technique  proposÈe par I. T. JOLLIFFE (2003), empruntant l'idÈe de  Tibshiran (1996) du lasso "the least absolute shrinkage and selection operation" qu'on applique d'habitude dans la rÈgresion multiple quand le nombre d'Èquation est ÈlevÈe et le problËme de l'interprÈtation se pose. Cette approche est nommÈ so-Called LASSO permet d'introduire une borne sur la somme des valeurs absolues des coefficients, ces derniers deviennent nul si ils sont infÈrieur ‡ cette borne.

Soit une ACP sur la matrice de corrÈlation qui donne une combinaision linÈaire $ a'_{k} x$, $(k=1, ...,p)$ des p variables x mesurÈes, qui ont une variance maximale de $a'_{k}* R * a_{k}$


### La deuxiËme mÈthode: SPCA

### La troisiËme mÈthode: PCA en utilisant la dÈcomposition de PMD



## Justification de la m√©thodologie
Des simulations des mÈthodes!!

## Application de la m√©thodologie

## Autres √©l√©ments pertinents

## Bibliographie
