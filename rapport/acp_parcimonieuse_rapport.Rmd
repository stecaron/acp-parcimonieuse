---
title: "ACP Parcimonieuse"
author: "St√f¬©phane Caron/Sofia Harrouch"
date: '2018-03-13'
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Introduction

Les m√©thodes statistiques de r√©duction de la dimensionnalit√© ont g√©n√©ralement comme objectif de r√©duire la dimension d'un jeu de donn√©es dans le but de simplifier l'interpr√©tation des donn√©es, de permettre la visualisation des donnn√©es ou m√™me de d'am√©liorer la performance de certaines m√©thodes appliqu√©es sur ces donn√©es r√©duites. En termes simples, r√©duire la dimensionnalit√© revient √† r√©duire le nombre de variables (p) mesur√©es.

L'analyse en composante principale est une m√©thode classique de r√©duction de la dimensionnalit√©. Cette m√©thode permet de cr√©er des combinaisons lin√©aires des diff√©rentes variables du jeu de donn√©es tout en conservant le plus de variabilit√© possible. Chacune des nouvelles composantes principales cr√©es poss√®dent un vecteur de coefficients de saturation (loadings) de dimension $p\text{ x }1$, correspondant en quelque sorte √† l'importance attribu√©e √† chacune des diff√©rentes variables originales du jeu de donn√©es. Il est donc possible d'interpr√©ter ces coefficients de saturation et d'obtenir une interpr√©tation plus g√©n√©ralis√©e de certaines composantes principales calcul√©es.

Cependant, cette interpr√©tation peut se r√©v√®ler assez complexe dans le cas o√π une composante principale est expliqu√©e (coefficients de saturation √©lev√©s) par plusieurs variables originales du jeu de donn√©es. De plus, il peut √™tre difficile de d√©finir √† partir de quelle valeur exactement un coefficient de saturation est consid√©r√© comme √©tant "non important" pour une composante principale. Pour palier √† ce probl√®me d'interpr√©tation, il existe diff√©rentes diff√©rentes m√©thodes connues. Par exemple, les rotations [@Jolliffe1989] cherche √† simplifier l'interpr√©tation des composantes principales. Il pourrait √©galement √™tre possible d'√©carter les coefficients de saturation inf√©rieurs √† une certaine valeur ou simplement restreindre les valeurs possibles que ces coefficients peuvent prendre (ex: -1, 0 ou 1). Ces m√©thodes sont des exemples de strat√©gie permetttant de faciliter l'interpr√©tation des composantes principales, mais elles ont tous certains d√©savantages.

La m√©thodologie introduite dans le pr√©sent document [@Jolliffe2003] est en quelque sorte une alternative √† ces m√©thodes. En bref, elle consiste √† ajouter certaines contraintes au mod√®le d'analyse en composante principale qui auront comme objectif d'am√©liorer l'interpr√©tabilit√© des composantes d√©riv√©es. Cela permettra notamment d'obtenir des coefficients de saturation exactement √©gale √† zero. On pourrait donc dire que cette m√©thode permet de combiner l'aspect r√©duction de la dimensionnalit√© apport√©e par l'ACP et l'aspect simplification de l'interpr√©tabilit√© apport√© par les exemples d√©crits plus haut. 

La section 2 fera l'illustration du genre de probl√®me qu'on peut √©prouver avec l'analyse en composante principale et les rotations en terme d'interpr√©tatibilit√©. La section 3 a comme objectif de d√©crire la m√©thodologie. Dans la section 4, nous verrons plus en d√©tails la justification th√©orique et les r√©sultats de simulation de la m√©thodologie. La section 5 permettra d'illustrer avec un exemple complet les r√©sultats de la m√©thodologie. Finalement, la section 6 aura comme but de conclure bri√®vement en plus de mentionner d'autres √©l√©ments √† savoir √† propos de la m√©thodologie.

## Exemple de motivation

Pour illustrer la motivation derr√®re la m√©thodologie, supposons qu'on cherche √† simplifier un jeu de donn√©es provenant d'un √©chantillon de 180 coupes de bois de pin afin d'avoir une meilleure comp√©hension des diff√©rentes mesures (variables) impliqu√©es. Les diff√©rentes variables du jeu de donn√©es en question sont pr√©sent√©es dans le tableau 1. √Ä partir de la matrice de corr√©lation, il est possible de commencer par faire l'analyse en composante principale et analyser les diff√©rentes composantes calcul√©es.

```{r dataset}
library(elasticnet)
library(data.table)
data(pitprops)
data_description <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops)))),
  description = c("Diam√®tre dans le haut de l'arbre (en pouces)", "Longeur (en pouces)", "Humidit√© (% poids sec)", "Gravit√© au moment du test", "Nombre d'anneaux dans le haut de l'arbre", "Nombre d'anneaux dans le haut de l'arbre", "Branche principale (en pouces)", "Distance du bout de la branche principale au haut de l'arbre", "Nombre de spires", "Longueur de l'h√©lice transparente du haut (en pouces)", "Nombre moyen de n≈ìuds par verticille", "Diam√®tre moyen des noeuds (en pouces)")
)
knitr::kable(x = data_description, col.names = c("Variables", "Description"), row.names = FALSE, align = c("c", "l"), caption = "Tableau 1: Pr√©sentation des diff√©rentes variables du jeu de donn√©es pitprops.")
```

Comme mentionn√© dans l'introduction, l'ACP consiste essentiellement √† trouver des combinaisons lin√©aires des variables originales du jeu donn√©es (disons la matrice $X$) tout en maximisant la variance. En termes plus th√©oriques, la premi√®re composante principale est calcul√©e en maximisant la fonction

$$
F(\alpha_1) = \alpha_1' \Sigma \alpha_1
$$
avec la contrainte que $\alpha_1'\alpha_1=1$. La matrice $\Sigma$ correspond √† la matrice de covariance ou √† la matrice de corr√©lation (d√©pend situation).

Le vecteur $\alpha_1$ correspond au vecteur de coefficients de saturation de la premi√®re composante principale. On refait la m√™me chose pour la deuxi√®me composante principale en ajoutant la contrainte que

$$
cov(\alpha_1'X, \alpha_2'X) = 0
$$
Il est √©galement possible de d√©montrer que si on fait la d√©composition en valeurs singuli√®res de la matrice de corr√©lation (ou covariance), on trouve que les vecteurs $\alpha1, ..., \alpha_p$ correspondent aux vecteurs propres norm√©s de la matrice $\Sigma$ alors que la variance de chacune des composantes principales correspond aux $p$ valeurs propres de la m√™me matrice $\Sigma$. Ainsi, apr√®s avoir fait la d√©composition en valeurs et vecteurs propres de la matrice de corr√©lation, il est possible d'analyser essentiellement deux choses:

1. L'interpr√©tabilit√© de chacune des composantes principales
2. L'information conserv√©e √† chacune des composantes principales

La premi√®re peut √™tre analys√©e en tentant d'interpr√©ter les coefficients de saturation (vecteurs propres). Pour l'exemple, nous allons quantifier cette composante avec un facteur de simplicit√© (varimax) qui prend des valeurs entre 0 et 1 o√π plus le facteur est faible, plus la composante est simple d'interpr√©tation. La deuxi√®me composante peut quant √† elle √™tre quantifi√©e avec la variance expliqu√©e par la composante principale.

Le tableau 2 montre les r√©sultats des 6 premi√®res composantes principales calcul√©s par l'ACP. On garde seulement les 6 premi√®res composantes √©tant donn√© qu'elles expliquent plus de 87% de la variabilit√© totale du jeu de donn√©es.

```{r acp}
options(scipen = 999)
acp <- prcomp(x = pitprops, center = FALSE)

data_pca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Simplicity", "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(acp$rotation[, 1]), 0, 100*acp$sdev[1]/sum(acp$sdev), 100*cumsum(acp$sdev)[1]/sum(acp$sdev)), 3),
  PC2 = round(c(as.numeric(acp$rotation[, 2]), 0, 100*acp$sdev[2]/sum(acp$sdev), 100*cumsum(acp$sdev)[2]/sum(acp$sdev)), 3),
  PC3 = round(c(as.numeric(acp$rotation[, 3]), 0, 100*acp$sdev[3]/sum(acp$sdev), 100*cumsum(acp$sdev)[3]/sum(acp$sdev)), 3),
  PC4 = round(c(as.numeric(acp$rotation[, 4]), 0, 100*acp$sdev[4]/sum(acp$sdev), 100*cumsum(acp$sdev)[4]/sum(acp$sdev)), 3),
  PC5 = round(c(as.numeric(acp$rotation[, 5]), 0, 100*acp$sdev[5]/sum(acp$sdev), 100*cumsum(acp$sdev)[5]/sum(acp$sdev)), 3),
  PC6 = round(c(as.numeric(acp$rotation[, 6]), 0, 100*acp$sdev[6]/sum(acp$sdev), 100*cumsum(acp$sdev)[6]/sum(acp$sdev)), 3)
)
knitr::kable(data_pca, row.names = FALSE, col.names = c("", paste0("PC", seq(1:6))), align = rep("c", 6), caption = "Tableau 2: Coefficients de saturation de l'analyse en composante principale effectu√©e sur la matrice de corr√©lation du jeu de donn√©es pitprops.")
```



```{r rotation}
library(psych)
acp_rotated <- principal(r = pitprops, 6, rotate = "varimax", eps = 1e-14)
```


## Description de la m√©thodologie
Comme mentionnÈ prÈcÈdemment, L'ACP souffre parfois du problËme d'interprÈtation des axes. Pour cela on a recours ‡ une nouvelle mÈthode **ACP parcimonieuse (SPARSE)** qui nous donne des axes "Sparses" expliquÈs par un petit nombre des variables; une mÈthode qui ignore l'effet de certaines variables sur les axes principales.
Dans cette section, on expliquera les diffÈrentes mÈthodes proposÈes pour l'estimation de ces axes. La premiËre mÈthode est basÈe sur la propriÈtÈ d'obtention d'une variance maximale des composantes principale(SCoTLASS), la deuxiËme mÈthode est construit en se basant sur la propriÈtÈ de l'erreur de recontruction/ de regression( SPCA) et la derniËre mÈthode est celle obtenue en utilisant la dÈcomposition PMD (PCA).

### L'approche du Lasso en ACP: SCoTLASS
Une technique  proposÈe par I. T. JOLLIFFE (2003), empruntant l'idÈe de  Tibshiran (1996) du lasso "the least absolute shrinkage and selection operation" qu'on applique d'habitude dans la rÈgresion multiple quand le nombre d'Èquation est ÈlevÈe et le problËme de l'interprÈtation se pose. Cette approche est nommÈ "Simplifed Component Technique LASSO"" permet d'introduire une borne sur la somme des valeurs absolues des coefficients, ces derniers deviennent nul s'ils sont infÈrieurs ‡ cette borne.

Soit X le jeu de donnÈes $X=(X_{1}, .., X_{p})^{T}$, sa matrice de corrÈlation $R= corr(X)$ 
Soit une ACP sur la matrice de corrÈlation qui donne les composantes principales qui sont des combinaisions linÈaires des p variables mesurÈes de  soit $Y_{k}= a'_{k} X = \sum_{i=1}^{p} a_{ki} X_{i}$, $(k=1, ...,p)$. On note ensuite la variance de l'axe principale $Y_{k}$ par $var(Y_{k})=a'_{k}* R * a_{k}$.
le problËme de maximisation de l'ACP pour conserver la plus grande quantitÈ d'information possible est:
$$
\begin{align}
\text{max  }  a'_{k}* R * a_{k} \\
s/c
\bigg\{
\begin{array}
aa'_{k}* a_{k} = 1, &(k \geq 2)\\
a'_{h}* a_{k} = 0, &(h \geq k)\\
\end{array}
\end{align}
$$


La mÈthode du lasso appliquÈ ‡ l'asso'ACP, rajoute une troisiËme contrainte sur les coefficients des variables sur les axes.
$$
\begin{align}
\sum_{j=1}^{p} |a_{kj}| \leq t
\end{align}
$$

Dans cette mÈthode, il n y a pas de rËgle ni d'orientation pour le choix de t, ce t qu' ‡ partir duquel on ignore la significativitÈ des variables initiales sur les axes. ‡ titre indicatif, on a:
1. pour t $\geq \sqrt{p} $, on a l'ACP.
2. pour t $\leq 1 $, il n'existe pas de solution.
3. pour t=1, on a exactement une valeur non nulle de $a_{kj}$ pour chaque k.
Donc on choisit des valeurs diffÈrentes de t  ce qui a comme consÈquence un co˚t ÈlevÈ de calcul, une problËme d'optimisation non convexe. Sans oublier aussi le fait que cette mÈthode ne nous fournisse pas vraiment des vecteurs propres assez sparses quand le besoin d'un grand pourcentage de la variance expliquÈe est exprimÈ, on se trouve donc dans une situation de compromis entre le pourcentage de variance expliquÈe et  l'interprÈtation des variables.


### SPCA: Sparse Principal Component Analysis
Dans cette partie, on introduit une autre approche estimant les composantes principales tout en rendant les vecteurs parcimonieux (sparse).
L'ACP peut Ítre rÈecrite comme un problËme d'optimisation d'une rÈgression en imposant une pÈnalitÈ quadratique: la pÈnalitÈ du Lasso via l'Elastic net.
En fait, chaque composante principale est Ècrite comme une combinaison linÈaire des p variables, donc les coefficients des variables sur les CPs peuvent Ítre obtenus en regressant la CPs sur ces p variables.
AprÈs l'application de l'ACP sur notre jeu de donnÈe, on reconstruit les facteurs (loadings) par une regression ridge, cette mÈthode dÈpend des rÈsultats obtenus de l'ACP(post ACP).

On note $Y_{i}$ la composante principale, $\lambda$ positive, l'estimateur de ridge est:
$$\hat{\beta}_{ridge}= arg min_{\beta} || Y_{i} - X\beta||^2 +\lambda ||\beta||^2 $$

Soit $\hat{v}=\frac{\hat{\beta}_{ridge}}{||\hat{\beta}_{ridge}||}$, donc $\hat{v}=V_{i}$
Cette pÈnalitÈ de ridge n'en n'est pas vraiment une, elle sert simplement ‡ reconstruire les composantes.

Ensuite, la mÈthode rajoute une nouvelle pÈnalitÈ $L_{1}$;, la pÈnalitÈ de Lasso, ‡ l'equation prÈcedante, ce qui donne un nouveau problËme d'optimisation:

$$\hat{\beta}= arg min_{\beta} || Y_{i} - X\beta||^2 +\lambda ||\beta||^2+\lambda_{1} ||\beta||_{1} $$

telle que: $||\beta||_{1}= \sum_{j=1}^{p}|\beta_{j}|$ est la norme 1 de $\beta$. On appelle $\hat{V}_{i}= \frac{\hat{\beta}}{||\hat{\beta}||}$ l'approximation de $V_{i}$, et la $X\hat{V}_{i}$ la iËme composante principale approximÈe.

Dans notre cas, on considËre les k premiers CPs. Soit $A_{p*k}=[\alpha_{1}, \alpha_{2}, ....,\alpha_{k}]$ et $B_{p*k}=[\beta_{1}, \beta_{2}, ....,\beta_{k}]$

Le problËme d'optimisation gÈnarlisÈ obtenu est:
$$(\hat{A},\hat{B})=argmin \sum_{i=1}^{n} ||X_{i} -AB^{T}X_{i}||^2 + \sum_{j=1}^{k} |\beta_{j}||^2 + \sum_{j=1}^{k} \lambda_{1,j} ||\beta_{j}||_{1} $$
sous la contrainte $$A^T A = I_{k*k}$$


## Justification de la m√©thodologie
## Justification de la m√©thodologie
DAns cette partie on va comparer la performance des deux mÈthodes ScotLASS et SPCA.
On considËre trois facteurs: 
$$
V_{1} \sim N(0,290) \\
V_{2} \sim N(0,300)\\
V_{3} = -0.3 * V_{1} +0.925 * V_{2} +\epsilon, \space \epsilon \sim N(0,1)
$$
tel que $V_{1},\space V_{2}, $et $\epsilon \sim N(0,1)$
On construit 10 variable tel que:
$$X_{i}=V_{1}+ \epsilon^1_{i}, \space \epsilon^1_{i} \sim N(0,1) \space \text{pour i=1, 2, 3, 4}\\
X_{i}=V_{2}+ \epsilon^2_{i}, \space \epsilon^2_{i} \sim N(0,1) \space \text{pour i=5, 6, 7, 8}\\
X_{i}=V_{3}+ \epsilon^3_{i}, \space \epsilon^3_{i} \sim N(0,1) \space \text{pour i=1,2 ,3, 4}\\
$$

On remarque que les deux premiers axes de l'ACP explique plus de 80% de la variance totale, et que 


```{r sim ACP SPCA Scotlass}
#simulation
rm(list=ls())

## ACP
v1 <- rnorm(1000,0,sqrt(290))
v2 <- rnorm(1000,0,sqrt(300))
v3 <- -0.3 * v1 + 0.925 * v2 + rnorm(1000)


x=matrix(0,1000,10)
for(i in 1:4){
  x[,i]= v1+ rep(rnorm(1),1000)
}

for(i in 5:8){
  x[,i]= v2+ rep(rnorm(1),1000)
}

for(i in 9:10){
  x[,i]= v3+ rep(rnorm(1),1000)
}
cor(x)
data.acp <- princomp(x)
dat.vecp = loadings(data.acp)  
print(dat.vecp, cutoff = 0)
summary(data.acp)


##SPCA
sparse.pca.result <- spca(x, K = 2, type = "predictor", sparse = "varnum", para = c(4, 4))
sparse.pca.result
```

## Application de la m√©thodologie

```{r spca}
rpca <- principal(pitprops, nfactors = 6, rotate = "varimax")
spca_pitprops <- spca(pitprops, K = 6, type = "predictor", sparse = "penalty", para = c(0.06, 0.16, 0.1, 0.1, 0.1, 0.1))
sapply(1:6, function(i) angle(acp$rotation[, i], spca_pitprops$loadings[, i]))
sapply(1:6, function(i) angle(acp$rotation[, i], rpca$loadings[, i]))
```


## Autres √©l√©ments pertinents

## Bibliographie
