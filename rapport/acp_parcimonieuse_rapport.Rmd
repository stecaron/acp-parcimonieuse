---
title: "ACP Parcimonieuse"
author: "St√©phane Caron/Sofia Harrouch"
date: '2018-03-13'
bibliography: bibliography.bib
output:
  pdf_document:
    toc: yes
    includes:
            in_header: packages.sty
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Introduction

Les m√©thodes statistiques de r√©duction de la dimensionnalit√© ont g√©n√©ralement comme objectif de r√©duire la dimension d'un jeu de donn√©es dans le but de simplifier l'interpr√©tation des donn√©es, de permettre la visualisation des donnn√©es ou m√™me de d'am√©liorer la performance de certaines m√©thodes appliqu√©es sur ces donn√©es r√©duites. En termes simples, r√©duire la dimensionnalit√© revient √† r√©duire le nombre de variables (p) mesur√©es.

L'analyse en composante principale est une m√©thode classique de r√©duction de la dimensionnalit√©. Cette m√©thode permet de cr√©er des combinaisons lin√©aires des diff√©rentes variables du jeu de donn√©es tout en conservant le plus de variabilit√© possible. Chacune des nouvelles composantes principales cr√©√©es poss√®dent un vecteur de coefficients de saturation (loadings) de dimension $p\text{ x }1$, correspondant en quelque sorte √† l'importance attribu√©e √† chacune des diff√©rentes variables originales du jeu de donn√©es. Il est donc possible d'interpr√©ter ces coefficients de saturation et d'obtenir une interpr√©tation plus g√©n√©ralis√©e de certaines composantes principales calcul√©es.

Cependant, cette interpr√©tation peut se r√©v√®ler assez complexe dans le cas o√π une composante principale est expliqu√©e (coefficients de saturation √©lev√©s) par plusieurs variables originales du jeu de donn√©es. De plus, il peut √™tre difficile de d√©finir √† partir de quelle valeur exactement un coefficient de saturation est consid√©r√© comme √©tant "non important" pour une composante principale. Pour palier √† ce probl√®me d'interpr√©tation, il existe diff√©rentes diff√©rentes m√©thodes connues. Par exemple, les rotations [@Jolliffe1989] cherche √† simplifier l'interpr√©tation des composantes principales. Il pourrait √©galement √™tre possible d'√©carter les coefficients de saturation inf√©rieurs √† une certaine valeur ou simplement de restreindre les valeurs possibles que ces coefficients peuvent prendre (ex: -1, 0 ou 1). Ces m√©thodes sont des exemples de strat√©gie permetttant de faciliter l'interpr√©tation des composantes principales, mais elles ont tous certains d√©savantages.

La m√©thodologie introduite dans le pr√©sent document [@Jolliffe2003] est en quelque sorte une alternative √† ces m√©thodes. En bref, elle consiste √† ajouter certaines contraintes au mod√®le d'analyse en composante principale qui auront comme objectif d'am√©liorer l'interpr√©tabilit√© des composantes calcul√©es. Cela permettra notamment d'obtenir des coefficients de saturation exactement √©gale √† zero. On pourrait donc dire que cette m√©thode permet de combiner l'aspect r√©duction de la dimensionnalit√© apport√©e par l'ACP et l'aspect simplification de l'interpr√©tabilit√© apport√© par les exemples d√©crits plus haut. 

La section 2 fera l'illustration du genre de probl√®me qu'on peut √©prouver avec l'analyse en composante principale et les rotations en terme d'interpr√©tatibilit√©. La section 3 a comme objectif de d√©crire la m√©thodologie. Dans la section 4, nous verrons plus en d√©tails la justification th√©orique et les r√©sultats de simulation de la m√©thodologie. La section 5 permettra d'illustrer avec un exemple complet les r√©sultats de la m√©thodologie. Finalement, la section 6 aura comme but de conclure bri√®vement en plus de mentionner d'autres √©l√©ments √† savoir √† propos de la m√©thodologie.

## Exemple de motivation

Pour illustrer la motivation derr√®re la m√©thodologie, supposons qu'on cherche √† simplifier un jeu de donn√©es provenant d'un √©chantillon de 180 coupes de bois de pin afin d'avoir une meilleure comp√©hension des diff√©rentes mesures (variables) impliqu√©es. Les diff√©rentes variables du jeu de donn√©es en question sont pr√©sent√©es dans le tableau 1. √Ä partir de la matrice de corr√©lation, il est possible de commencer par faire l'analyse en composante principale et analyser les diff√©rentes composantes calcul√©es.

```{r dataset}
library(elasticnet)
library(data.table)
data(pitprops)
data_description <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops)))),
  description = c("Diam√®tre dans le haut de l'arbre (en pouces)", "Longeur (en pouces)", "Humidit√© (% poids sec)", "Gravit√© au moment du test", "Nombre d'anneaux dans le haut de l'arbre", "Nombre d'anneaux dans le haut de l'arbre", "Branche principale (en pouces)", "Distance du bout de la branche principale au haut de l'arbre", "Nombre de spires", "Longueur de l'h√©lice transparente du haut (en pouces)", "Nombre moyen de n≈ìuds par verticille", "Diam√®tre moyen des noeuds (en pouces)")
)
knitr::kable(x = data_description, col.names = c("Variables", "Description"), row.names = FALSE, align = c("c", "l"), caption = "Pr√©sentation des diff√©rentes variables du jeu de donn√©es pitprops.")
```

Comme mentionn√© dans l'introduction, l'ACP consiste essentiellement √† trouver des combinaisons lin√©aires des variables originales du jeu donn√©es (disons la matrice $X$) tout en maximisant la variance. En termes plus th√©oriques, la premi√®re composante principale est calcul√©e en maximisant la fonction

$$
F(\alpha_1) = \alpha_1' \Sigma \alpha_1
$$
avec la contrainte que $\alpha_1'\alpha_1=1$. La matrice $\Sigma$ correspond √† la matrice de covariance ou √† la matrice de corr√©lation (d√©pend situation).

Le vecteur $\alpha_1$ correspond au vecteur de coefficients de saturation de la premi√®re composante principale. On refait la m√™me chose pour la deuxi√®me composante principale en ajoutant la contrainte que

$$
cov(\alpha_1'X, \alpha_2'X) = 0
$$
Il est √©galement possible de d√©montrer que si on fait la d√©composition en valeurs singuli√®res de la matrice de corr√©lation (ou covariance), on trouve que les vecteurs $\alpha_1, ..., \alpha_p$ correspondent aux vecteurs propres norm√©s de la matrice $\Sigma$ alors que la variance de chacune des composantes principales correspond aux $p$ valeurs propres de la m√™me matrice $\Sigma$. Ainsi, apr√®s avoir fait la d√©composition en valeurs et vecteurs propres de la matrice de corr√©lation, il est possible d'analyser essentiellement deux choses:

1. L'interpr√©tabilit√© de chacune des composantes principales
2. L'information conserv√©e √† chacune des composantes principales

La premi√®re peut √™tre analys√©e en tentant d'interpr√©ter les coefficients de saturation (vecteurs propres). Plus il y a de coefficients similaires, plus la composante est difficile √† interpr√©ter. √Ä l'extr√™me, le cas le plus simple serait le cas o√π seulement un seul coefficient ne serait pas √©gale √† 0. Dans ce cas-ci, la composante serait effectivement facile √† interpr√©ter, mais il y aurait probablement beaucoup de perte d'information (peu de variance exepliqu√©e), ce qui n'est pas n√©c√©sssairement d√©sir√©. L'information conserv√©e √† chacune des composantes peut quant √† elle √™tre quantifi√©e avec la variance expliqu√©e par la composante principale.

Le tableau 2 montre les r√©sultats des 6 premi√®res composantes principales calcul√©s par l'ACP. On garde seulement les 6 premi√®res composantes √©tant donn√© qu'elles expliquent plus de 87% de la variabilit√© totale du jeu de donn√©es.

```{r acp}
options(scipen = 999)
acp <- prcomp(x = pitprops, center = FALSE)

data_pca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(acp$rotation[, 1]), 100*acp$sdev[1]/sum(acp$sdev), 100*cumsum(acp$sdev)[1]/sum(acp$sdev)), 3),
  PC2 = round(c(as.numeric(acp$rotation[, 2]), 100*acp$sdev[2]/sum(acp$sdev), 100*cumsum(acp$sdev)[2]/sum(acp$sdev)), 3),
  PC3 = round(c(as.numeric(acp$rotation[, 3]), 100*acp$sdev[3]/sum(acp$sdev), 100*cumsum(acp$sdev)[3]/sum(acp$sdev)), 3),
  PC4 = round(c(as.numeric(acp$rotation[, 4]), 100*acp$sdev[4]/sum(acp$sdev), 100*cumsum(acp$sdev)[4]/sum(acp$sdev)), 3),
  PC5 = round(c(as.numeric(acp$rotation[, 5]), 100*acp$sdev[5]/sum(acp$sdev), 100*cumsum(acp$sdev)[5]/sum(acp$sdev)), 3),
  PC6 = round(c(as.numeric(acp$rotation[, 6]), 100*acp$sdev[6]/sum(acp$sdev), 100*cumsum(acp$sdev)[6]/sum(acp$sdev)), 3)
)
knitr::kable(data_pca, row.names = FALSE, col.names = c("", paste0("PC", seq(1:6))), align = rep("c", 6), caption = "Tableau 2: Coefficients de saturation de l'analyse en composante principale effectu√©e sur la matrice de corr√©lation du jeu de donn√©es pitprops.")
```

Dans le tableau 2, on remarque que les premi√®res composantes principales ont beaucoup de coefficients qui se ressemblent, ce qui rend difficile l'interpr√©tation de celles-ci. Pour palier √† ce probl√®me, nous pouvons effectuer une rotation de ces composantes principales. Une rotation classique dans ce genre de situation serait la rotation varimax. Cette rotation est de type orthogonale, c'est donc dire que le syst√®me de coordonn√©es actuel ne subit seulement qu'une rotation. La rotation est faite dans le but de rapprocher le plus possible les coefficients de saturation vers 0 ou 1. Le tableau 3 montre les r√©sultats obtenus apr√®s avoir effectu√© la rotation varimax.

```{r rotation}
library(psych)

# acp_rotated <- principal(pitprops, 6, rotate = "varimax", eps = 1e-14)
# acp_rotated <- principal(r = pitprops, 6, rotate = "none", eps = 1e-14)

# acp_rotated
# 
# data_rpca <- data.table(
#   variable = c(paste0("x", seq(1:ncol(pitprops))), "Variance (%)", "Cumulative variance (%)"),
#   PC1 = round(c(as.numeric(acp_rotated_rotated$loadings[, 1]), 100*acp_rotated$sdev[1]/sum(acp_rotated$sdev), 100*cumsum(acp_rotated$sdev)[1]/sum(acp_rotated$sdev)), 3),
#   PC2 = round(c(as.numeric(acp_rotated$loadings[, 2]), 100*acp_rotated$sdev[2]/sum(acp_rotated$sdev), 100*cumsum(acp_rotated$sdev)[2]/sum(acp_rotated$sdev)), 3),
#   PC3 = round(c(as.numeric(acp_rotated$loadings[, 3]), 100*acp_rotated$sdev[3]/sum(acp_rotated$sdev), 100*cumsum(acp_rotated$sdev)[3]/sum(acp_rotated$sdev)), 3),
#   PC4 = round(c(as.numeric(acp_rotated$loadings[, 4]), 100*acp_rotated$sdev[4]/sum(acp_rotated$sdev), 100*cumsum(acp_rotated$sdev)[4]/sum(acp_rotated$sdev)), 3),
#   PC5 = round(c(as.numeric(acp_rotated$loadings[, 5]), 100*acp_rotated$sdev[5]/sum(acp_rotated$sdev), 100*cumsum(acp_rotated$sdev)[5]/sum(acp_rotated$sdev)), 3),
#   PC6 = round(c(as.numeric(acp_rotated$loadings[, 6]), 100*acp_rotated$sdev[6]/sum(acp_rotated$sdev), 100*cumsum(acp_rotated$sdev)[6]/sum(acp_rotated$sdev)), 3)
# )
# knitr::kable(data_pca, row.names = FALSE, col.names = c("", paste0("PC", seq(1:6))), align = rep("c", 6), caption = "Tableau 2: Coefficients de saturation de l'analyse en composante principale effectu√©e sur la matrice de corr√©lation du jeu de donn√©es pitprops.")
```



## Description de la m√©thodologie
Comme mentionn? pr?c?demment, L'ACP souffre parfois du probl?me d'interpr?tation des axes. Pour cela on a recours ? une nouvelle m?thode **ACP parcimonieuse (SPARSE)** qui nous donne des axes "Sparses" expliqu?s par un petit nombre des variables; une m?thode qui ignore l'effet de certaines variables sur les axes principales.
Dans cette section, on expliquera les diff?rentes m?thodes propos?es pour l'estimation de ces axes. La premi?re m?thode est bas?e sur la propri?t? d'obtention d'une variance maximale des composantes principale(SCoTLASS), la deuxi?me m?thode est construit en se basant sur la propri?t? de l'erreur de recontruction/ de regression( SPCA) et la derni?re m?thode est celle obtenue en utilisant la d?composition PMD (PCA).

### L'approche du Lasso en ACP: SCoTLASS
Une technique  propos?e par I. T. JOLLIFFE (2003), empruntant l'id?e de  Tibshiran (1996) du lasso "the least absolute shrinkage and selection operation" qu'on applique d'habitude dans la r?gresion multiple quand le nombre d'?quation est ?lev?e et le probl?me de l'interpr?tation se pose. Cette approche est nomm? "Simplifed Component Technique LASSO"" permet d'introduire une borne sur la somme des valeurs absolues des coefficients, ces derniers deviennent nul s'ils sont inf?rieurs ? cette borne.

Soit X le jeu de donn?es $X=(X_{1}, .., X_{p})^{T}$, sa matrice de corr?lation $R= corr(X)$ 
Soit une ACP sur la matrice de corr?lation qui donne les composantes principales qui sont des combinaisions lin?aires des p variables mesur?es de  soit $Y_{k}= a'_{k} X = \sum_{i=1}^{p} a_{ki} X_{i}$, $(k=1, ...,p)$. On note ensuite la variance de l'axe principale $Y_{k}$ par $var(Y_{k})=a'_{k}* R * a_{k}$.
le probl?me de maximisation de l'ACP pour conserver la plus grande quantit? d'information possible est:
$$
\text{max  }  a'_{k}* R * a_{k}
$$

$$
s/c
\bigg\{
  \begin{array}{lr}
    aa'_{k} \times a_{k} = 1, &(k \geq 2) \\
    a'_{h} \times a_{k} = 0, &(h \geq k) \\
  \end{array}
$$


La m?thode du lasso appliqu? ? l'asso'ACP, rajoute une troisi?me contrainte sur les coefficients des variables sur les axes.
$$
\sum_{j=1}^{p} |a_{kj}| \leq t
$$

Dans cette m?thode, il n y a pas de r?gle ni d'orientation pour le choix de t, ce t qu' ? partir duquel on ignore la significativit? des variables initiales sur les axes. ? titre indicatif, on a:
1. pour t $\geq \sqrt{p}$, on a l'ACP.
2. pour t $\leq 1$, il n'existe pas de solution.
3. pour t=1, on a exactement une valeur non nulle de $a_{kj}$ pour chaque k.
Donc on choisit des valeurs diff?rentes de t  ce qui a comme cons?quence un co?t ?lev? de calcul, une probl?me d'optimisation non convexe. Sans oublier aussi le fait que cette m?thode ne nous fournisse pas vraiment des vecteurs propres assez sparses quand le besoin d'un grand pourcentage de la variance expliqu?e est exprim?, on se trouve donc dans une situation de compromis entre le pourcentage de variance expliqu?e et  l'interpr?tation des variables.


### SPCA: Sparse Principal Component Analysis
Dans cette partie, on introduit une autre approche estimant les composantes principales tout en rendant les vecteurs parcimonieux (sparse).
L'ACP peut ?tre r?ecrite comme un probl?me d'optimisation d'une r?gression en imposant une p?nalit? quadratique: la p?nalit? du Lasso via l'Elastic net.
En fait, chaque composante principale est ?crite comme une combinaison lin?aire des p variables, donc les coefficients des variables sur les CPs peuvent ?tre obtenus en regressant la CPs sur ces p variables.
Apr?s l'application de l'ACP sur notre jeu de donn?e, on reconstruit les facteurs (loadings) par une regression ridge, cette m?thode d?pend des r?sultats obtenus de l'ACP(post ACP).

On note $Y_{i}$ la composante principale, $\lambda$ positive, l'estimateur de ridge est:
$$\hat{\beta}_{ridge}= arg min_{\beta} || Y_{i} - X\beta||^2 +\lambda ||\beta||^2 $$

Soit $\hat{v}=\frac{\hat{\beta}_{ridge}}{||\hat{\beta}_{ridge}||}$, donc $\hat{v}=V_{i}$
Cette p?nalit? de ridge n'en n'est pas vraiment une, elle sert simplement ? reconstruire les composantes.

Ensuite, la m?thode rajoute une nouvelle p?nalit? $L_{1}$;, la p?nalit? de Lasso, ? l'equation pr?cedante, ce qui donne un nouveau probl?me d'optimisation:

$$\hat{\beta}= arg min_{\beta} || Y_{i} - X\beta||^2 +\lambda ||\beta||^2+\lambda_{1} ||\beta||_{1} $$

telle que: $||\beta||_{1}= \sum_{j=1}^{p}|\beta_{j}|$ est la norme 1 de $\beta$. On appelle $\hat{V}_{i}= \frac{\hat{\beta}}{||\hat{\beta}||}$ l'approximation de $V_{i}$, et la $X\hat{V}_{i}$ la i?me composante principale approxim?e.

Dans notre cas, on consid?re les k premiers CPs. Soit $A_{p*k}=[\alpha_{1}, \alpha_{2}, ....,\alpha_{k}]$ et $B_{p*k}=[\beta_{1}, \beta_{2}, ....,\beta_{k}]$

Le probl?me d'optimisation g?narlis? obtenu est:
$$(\hat{A},\hat{B})=argmin \sum_{i=1}^{n} ||X_{i} -AB^{T}X_{i}||^2 + \sum_{j=1}^{k} |\beta_{j}||^2 + \sum_{j=1}^{k} \lambda_{1,j} ||\beta_{j}||_{1} $$
sous la contrainte $$A^T A = I_{k*k}$$
Pour rÈsumer, l'algorithme de cette mÈthode peut se rÈsumer comme suit:
1. On commence par $A = V[,1:k]$ les coefficients de saturation des K premieres composantes principales.
2. Sachant $A= [\alpha_{1}, \alpha_{2}, ...,\alpha_{k}]$, On rÈsout le problËme pour$j=1, 2, ..., k$
$$\beta_j =arg min_ \beta(\alpha_j-\beta)^TX^TX(\alpha_j-\beta)+\lambda||\beta||^2 +\lambda_1,j||\beta||_1$$
3. Pour une matrice $B=[\beta_1, \beta_2, ..., \beta_k]$ fixÈ, on calcule la dÈcomposition de la matrice en valeur singuliËre SVD de $X^TXB= UDV^T$, par la suite on met A ‡ jour, ce qui donne $A= UV^T$
4. On rÈpËte les Ètapes 2-3 jusqu'‡ la convergence.
5. On normalise V, $\hat V_j = \frac{\beta_j}{||\beta_j||}$

####Notes:
1. les rÈsultats de cet algorithme ne change pas vraiment ‡ la variation de$\lambda$, en pratique, on choisi $\lambda$ un petit nombre positive pour Èviter les problËmes potentiels de collinÈaritÈ dans X.

2. En principe, on peut choisir diffÈrentes combinaisons de 
$\{ \lambda_1,j\}$ pour avoir un bon choix des paramËtres et par l‡ avoir un bon compromis entre la variance expliquÈe et la sparsitÈ.

## Justification de la m√©thodologie
Dans cette partie on va comparer la performance des deux mÈthodes ScotLASS et SPCA.
On considËre trois facteurs: 
$$
V_{1} \sim N(0,290) \\
V_{2} \sim N(0,300)\\
V_{3} = -0.3 * V_{1} +0.925 * V_{2} +\epsilon, \space \epsilon \sim N(0,1)
$$
tel que $V_{1},\space V_{2}, $et $\epsilon \sim N(0,1)$
On construit 10 variable tel que:
$$X_{i}=V_{1}+ \epsilon^1_{i}, \space \epsilon^1_{i} \sim N(0,1) \space \text{pour i=1, 2, 3, 4,}\\
X_{i}=V_{2}+ \epsilon^2_{i}, \space \epsilon^2_{i} \sim N(0,1) \space \text{pour i=5, 6, 7, 8,}\\
X_{i}=V_{3}+ \epsilon^3_{i}, \space \epsilon^3_{i} \sim N(0,1) \space \text{pour i=9, 10.}\\
\{\epsilon^j_i\}  \text{sont indÈpendants,pour j=1, 2, 3 et i=1, ..., 10.}
$$

On remarque que les deux premiers axes de l'ACP expliquent plus de 80% de la variance totale, cela suggËre qu'on a besoin de deux facteurs dÈrivÈes avec une meilleur reprÈsentation parcimonieuse. Le nombre de variable associÈ ‡ chaqu'un des facteurs est respectivement 4, 4, 2. IdÈalement, Le premier facteur dÈrivÈ devra Ítre expliquÈ par $(X_5, X_6, X_7, X_8)$, alors que le deuxiËme devra Ítre expliquÈ par les variables $(X_1, X_2, X_3, X_4)$. En fait, si on rÈsout le problËme de maximisation de la variance sous la contrainte qu'il y a juste 4 variables qui ont des coefficient de saturantion non nulles, on obtient les mÍmes variables.
On remarque que la mÈthode SPCA donne des axes sparces et facilite l'interpretation des axes mais par consÈquent il y a une diminution du pourcentage de la variance expliquÈe.


```{r sim ACP SPCA Scotlass}
#simulation
rm(list=ls())

## ACP
v1 <- rnorm(1000,0,sqrt(290))
v2 <- rnorm(1000,0,sqrt(300))
v3 <- -0.3 * v1 + 0.925 * v2 + rnorm(1000)


x=matrix(0,1000,10)
for(i in 1:4){
  x[,i]= v1+ rep(rnorm(1),1000)
}

for(i in 5:8){
  x[,i]= v2+ rep(rnorm(1),1000)
}

for(i in 9:10){
  x[,i]= v3+ rep(rnorm(1),1000)
}
cor(x)
data.acp <- princomp(x)
dat.vecp = loadings(data.acp)  
print(dat.vecp, cutoff = 0)
summary(data.acp)


##SPCA
sparse.pca.result <- spca(x, K = 2, type = "predictor", sparse = "varnum", para = c(4, 4))
sparse.pca.result


```

## Application de la m√©thodologie
Cette partie a comme but de traiter la base de donnÈe classique "pitprops" qui montre la dificultÈ d'interpretation des composantes principales, et essayer de rÈgler ce problËme  avec la nouvelle mÈthode de l'ACP parcimonieuse en utilisant la technique SPCA.
On considËre les six composantes principales. On pose $\lambda=0$, $\lambda_1= (0.06, 0.16, 0.1, 0.5, 0.5, 0.5)$. Ces valeurs ont ÈtÈ choisi en se basant sur la contrainte d'obtenir ‡ peu prËs la mÍme variance expliquÈe dans l'ACP que dans SPCA, comme expliquÈ dans figure 1; 

```{r lambda}
##
```



```{r spca}
# rpca <- principal(pitprops, nfactors = 6, rotate = "varimax")
# spca_pitprops <- spca(pitprops, K = 6, type = "predictor", sparse = "penalty", para = c(0.06, 0.16, 0.1, 0.1, 0.1, 0.1))
# sapply(1:6, function(i) angle(acp$rotation[, i], spca_pitprops$loadings[, i]))
# sapply(1:6, function(i) angle(acp$rotation[, i], rpca$loadings[, i]))
```


## Autres √©l√©ments pertinents

## Bibliographie
