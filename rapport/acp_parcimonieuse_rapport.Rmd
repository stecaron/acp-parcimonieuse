---
title: "ACP Parcimonieuse"
author: "St√f¬©phane Caron/Sofia Harrouch"
date: '2018-04-04'
output:
  word_document:
    toc: yes
  pdf_document:
    toc: yes
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## 1. Introduction

Les m√©thodes statistiques de r√©duction de la dimensionnalit√© ont g√©n√©ralement comme objectif de r√©duire la dimension d'un jeu de donn√©es dans le but de simplifier l'interpr√©tation des donn√©es, de permettre la visualisation des donn√©es ou m√™me d'am√©liorer la performance de certaines m√©thodes appliqu√©es sur ces donn√©es r√©duites. En termes simples, r√©duire la dimensionnalit√© revient √† r√©duire le nombre de variables (p) mesur√©es.

L'analyse en composantes principales est une m√©thode classique de r√©duction de la dimensionnalit√©. Cette m√©thode permet de cr√©er des combinaisons lin√©aires des diff√©rentes variables du jeu de donn√©es tout en conservant le plus de variabilit√© possible. Chacune des nouvelles composantes principales cr√©√©es poss√®de un vecteur de coefficients de saturation (loadings) de dimension $p\text{ x }1$, correspondant en quelque sorte √† l'importance attribu√©e √† chacune des diff√©rentes variables originales du jeu de donn√©es. Il est donc possible d'interpr√©ter ces coefficients de saturation et d'obtenir une interpr√©tation plus g√©n√©ralis√©e de certaines composantes principales calcul√©es.

Cependant, cette interpr√©tation peut se r√©v√©ler assez complexe dans le cas o√π une composante principale est expliqu√©e (coefficients de saturation √©lev√©s) par plusieurs variables originales du jeu de donn√©es. De plus, il peut √™tre difficile de d√©finir √† partir de quelle valeur exactement un coefficient de saturation est consid√©r√© comme √©tant "non important" pour une composante principale. Pour palier √† ce probl√®me d'interpr√©tation, il existe diff√©rentes m√©thodes connues. Par exemple, les rotations [@Jolliffe1989] cherchent √† simplifier l'interpr√©tation des composantes principales. Il pourrait √©galement √™tre possible d'√©carter les coefficients de saturation inf√©rieurs √† une certaine valeur ou simplement de restreindre les valeurs possibles que ces coefficients peuvent prendre (ex: -1, 0 ou 1). Ces m√©thodes sont des exemples de strat√©gie permettant de faciliter l'interpr√©tation des composantes principales, mais elles ont toutes certains d√©savantages.

La m√©thodologie introduite dans le pr√©sent document est en quelque sorte une alternative √† ces m√©thodes. En bref, elle consiste √† ajouter certaines contraintes au mod√®le d'analyse en composantes principales qui auront comme objectif d'am√©liorer l'interpr√©tabilit√© des composantes calcul√©es. Cela permettra notamment d'obtenir des coefficients de saturation exactement √©gales √† z√©ro. On pourrait donc dire que cette m√©thode permet de combiner l'aspect de r√©duction de la dimensionnalit√© apport√©e par l'ACP et l'aspect de simplification de l'interpr√©tabilit√© apport√© par les exemples d√©crits plus haut. 

La section 2 fera l'illustration du genre de probl√®me qu'on peut √©prouver avec l'analyse en composante principale et les rotations en terme d'interpr√©tatibilit√©. La section 3 a comme objectif de d√©crire la m√©thodologie. Dans la section 4, nous verrons plus en d√©tails la justification th√©orique et les r√©sultats de simulation de la m√©thodologie. La section 5 permettra d'illustrer avec un exemple complet les r√©sultats de la m√©thodologie.


## 2. Exemple de motivation

Pour illustrer la motivation derri√®re la m√©thodologie, supposons qu'on cherche √† simplifier un jeu de donn√©es provenant d'un √©chantillon de 180 coupes de bois de pin afin d'avoir une meilleure compr√©hension des diff√©rentes mesures (variables) impliqu√©es. Les diff√©rentes variables du jeu de donn√©es en question sont pr√©sent√©es dans le tableau 1. √Ä partir de la matrice de corr√©lation, il est possible de commencer par faire l'analyse en composante principale et analyser les diff√©rentes composantes calcul√©es.

```{r import_data, echo = TRUE}
library(elasticnet)

# Ce jeu de donn√©es correspond √† la matrice de corr√©lation
data(pitprops)
```


```{r dataset}
library(data.table)

data_description <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops)))),
  description = c("Diam√®tre dans le haut de l'arbre (en pouces)", "Longeur (en pouces)", "Humidit√© (% poids sec)", "Gravit√© au moment du test", "Nombre d'anneaux dans le haut de l'arbre", "Nombre d'anneaux dans le bas de l'arbre", "Branche principale (en pouces)", "Distance du bout de la branche principale au haut de l'arbre", "Nombre de spires", "Longueur de l'h√©lice transparente du haut (en pouces)", "Nombre moyen de n≈ìuds par verticille", "Diam√®tre moyen des noeuds (en pouces)")
)
knitr::kable(x = data_description, col.names = c("Variables", "Description"), row.names = FALSE, align = c("c", "l"), caption = "Pr√©sentation des diff√©rentes variables du jeu de donn√©es pitprops.")
```

Comme mentionn√© dans l'introduction, l'ACP consiste essentiellement √† trouver des combinaisons lin√©aires des variables originales du jeu donn√©es (disons la matrice $X$) tout en maximisant la variance. En termes plus th√©oriques, la premi√®re composante principale est calcul√©e en maximisant la fonction

$$
F(\alpha_1) = \alpha_1' \Sigma \alpha_1
$$
avec la contrainte que $\alpha_1'\alpha_1=1$. La matrice $\Sigma$ correspond √† la matrice de covariance ou √† la matrice de corr√©lation (d√©pend de la situation).

Le vecteur $\alpha_1$ correspond au vecteur de coefficients de saturation de la premi√®re composante principale. On refait la m√™me chose pour la deuxi√®me composante principale en ajoutant la contrainte que:

$$
cov(\alpha_1'X, \alpha_2'X) = 0
$$
Il est √©galement possible de d√©montrer que si on fait la d√©composition en valeurs singuli√®res de la matrice de corr√©lation (ou covariance), on trouve que les vecteurs $\alpha_1, ..., \alpha_p$ correspondent aux vecteurs propres norm√©s de la matrice $\Sigma$ alors que la variance de chacune des composantes principales correspond aux $p$ valeurs propres de la m√™me matrice $\Sigma$. Ainsi, apr√®s avoir fait la d√©composition en valeurs et vecteurs propres de la matrice de corr√©lation, il est possible d'analyser essentiellement deux choses:

1. L'interpr√©tabilit√© de chacune des composantes principales
2. L'information conserv√©e √† chacune des composantes principales

La premi√®re peut √™tre analys√©e en tentant d'interpr√©ter les coefficients de saturation (vecteurs propres). Plus il y a de coefficients similaires, plus la composante est difficile √† interpr√©ter. √Ä l'extr√™me, le cas le plus simple serait le cas o√π seulement un seul coefficient ne serait pas √©gale √† 0. Dans ce cas-ci, la composante serait effectivement facile √† interpr√©ter, mais il y aurait probablement beaucoup de perte d'information (peu de variance expliqu√©e), ce qui n'est pas n√©cessairement d√©sir√©. L'information conserv√©e √† chacune des composantes peut quant √† elle √™tre quantifi√©e avec la variance expliqu√©e par la composante principale.

Le tableau 2 montre les r√©sultats des 6 premi√®res composantes principales calcul√©es par l'ACP. On garde seulement les 6 premi√®res composantes √©tant donn√© qu'elles expliquent plus de 87% de la variabilit√© totale du jeu de donn√©es.

```{r acp, echo = TRUE}
library(stats)

# Faire l'ACP classique sur la matrice de corr√©lation
acp <- prcomp(x = pitprops, center = FALSE)
```



```{r acp_results}
options(scipen = 999)
library(dplyr)
library(kableExtra)

data_pca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(acp$rotation[, 1]), 100*acp$sdev[1]/sum(acp$sdev), 100*cumsum(acp$sdev)[1]/sum(acp$sdev)), 3),
  PC2 = round(c(as.numeric(acp$rotation[, 2]), 100*acp$sdev[2]/sum(acp$sdev), 100*cumsum(acp$sdev)[2]/sum(acp$sdev)), 3),
  PC3 = round(c(as.numeric(acp$rotation[, 3]), 100*acp$sdev[3]/sum(acp$sdev), 100*cumsum(acp$sdev)[3]/sum(acp$sdev)), 3),
  PC4 = round(c(as.numeric(acp$rotation[, 4]), 100*acp$sdev[4]/sum(acp$sdev), 100*cumsum(acp$sdev)[4]/sum(acp$sdev)), 3),
  PC5 = round(c(as.numeric(acp$rotation[, 5]), 100*acp$sdev[5]/sum(acp$sdev), 100*cumsum(acp$sdev)[5]/sum(acp$sdev)), 3),
  PC6 = round(c(as.numeric(acp$rotation[, 6]), 100*acp$sdev[6]/sum(acp$sdev), 100*cumsum(acp$sdev)[6]/sum(acp$sdev)), 3)
)
knitr::kable(data_pca, format = "latex", booktabs = T, row.names = FALSE, col.names = c("", paste0("PC", seq(1:6))), align = rep("c", 6), caption = "Coefficients de saturation de l'analyse en composante principale effectu√©e sur la matrice de corr√©lation du jeu de donn√©es pitprops.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

Dans le tableau 2, on remarque que les premi√®res composantes principales ont beaucoup de coefficients qui se ressemblent, ce qui rend difficile l'interpr√©tation de celles-ci. Pour palier √† ce probl√®me, nous pouvons effectuer une rotation de ces composantes principales. Une rotation classique dans ce genre de situation serait la rotation varimax. Cette rotation est de type orthogonale, c'est donc dire que le syst√®me de coordonn√©es actuel ne subit seulement qu'une rotation. La rotation est faite dans le but de rapprocher le plus possible les coefficients de saturation vers 0 ou 1. Le tableau 3 montre les r√©sultats obtenus apr√®s avoir effectu√© la rotation varimax.

```{r rotation, echo = TRUE}
library(psych)

# Faire l'ACP, mais en faisant une rotation varimax des CP
acp_rotated <- principal(pitprops, 6, rotate = "varimax", eps = 1e-14)
```


```{r rotation_results}
data_rpca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(acp_rotated$loadings[, 1]), 0.28, 0.28), 3),
  PC2 = round(c(as.numeric(acp_rotated$loadings[, 2]), 0.15, 0.43), 3),
  PC3 = round(c(as.numeric(acp_rotated$loadings[, 3]), 0.12, 0.56), 3),
  PC4 = round(c(as.numeric(acp_rotated$loadings[, 4]), 0.12, 0.67), 3),
  PC5 = round(c(as.numeric(acp_rotated$loadings[, 5]), 0.11, 0.78), 3),
  PC6 = round(c(as.numeric(acp_rotated$loadings[, 6]), 0.09, 0.87), 3)
)
knitr::kable(data_rpca, format = "latex", booktabs = T, row.names = FALSE, col.names = c("", paste0("PC", c(1,2,3,6,5,4))), align = rep("c", 6), caption = "Coefficients de saturation de l'analyse en composante principale effectu√©e sur la matrice de corr√©lation du jeu de donn√©es pitprops apr√®s avoir effectu√© une rotation orthogonale 'varimax'.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

En analysant de plus pr√®s le tableau 3, on remarque que la rotation effectu√©e a permis d'am√©liorer l√©g√®rement l'interpr√©tabilit√© des premi√®res composantes. D√©sormais, on remarque que les variables x1 et x2 se d√©marquent davantage des autres dans la premi√®re composante, m√™me chose pour x3 et x4 dans la deuxi√®me composante. Bien que la rotation permet d'am√©liorer l'interpr√©tabilit√©, on remarque qu'on perd de la variabilit√© dans les premi√®res composantes apr√®s la rotation. Dans l'ACP classique, les 3 premi√®res composantes expliquaient environ 65% de la variabilit√© alors que dans le cas de l'ACP avec rotation, les 3 m√™mes composantes expliquent environ 56% de la variabilit√©. De plus, on remarque que ce ne sont plus n√©cessairement les m√™mes composantes qui expliquent successivement le maximum de variabilit√©. Par exemple, la 4√®me composante principale dans l'ACP classique est celle qui explique le moins de variabilit√© apr√®s la rotation. Ces constats sont en quelque sorte les inconv√©nients pouvant √™tre rattach√©s aux rotations et sont √©galement la motivation derri√®re l'ACP parcimonieuse.

## 3. Description de la m√©thodologie

Comme mentionn√© pr√©c√©demment, l'ACP souffre parfois du probl√®me d'interpr√©tation des axes, ou des vecteurs de coefficients de saturation. Pour palier √† ce probl√®me, il est possible d'avoir recours √† une m√©thode sp√©cifique, soit l'ACP parcimonieuse. Cette m√©thode permet de trouver des axes "√©parses" qui sont expliqu√©s par un petit nombre des variables seulement. On pourrait donc dire que la m√©thode fait en quelque sorte une r√©gularisation et ignore l'effet de certaines variables sur les axes principals.

Dans cette section, on expliquera les diff√©rentes m√©thodes propos√©es pour l'estimation de vecteurs de coefficients de saturation. La premi√®re m√©thode est bas√©e sur la propri√©t√© d'obtention d'une variance maximale des composantes principales (SCoTLASS) alors que la la deuxi√®me m√©thode est construite en se basant sur la propri√©t√© de l'erreur de reconstruction (SPCA).

### M√©thode 1: SCoTLASS

Cette technique [@Jolliffe2003], emprunte l'id√©e du LASSO (Least Absolute Shrinkage and Selection Operation) introduite par Tibshirani (1996). Le LASSO est g√©n√©ralement appliqu√© en r√©gression multiple quand le nombre de variables est √©lev√© en faisant une r√©gularisation sur celles-ci. Dans le probl√®me de l'interpr√©tabilit√© de l'ACP, cette m√©thode peut se r√©v√©ler fort utile pour garder uniquement les variables importantes et ainsi faciliter l'interpr√©tation. La m√©thode SCoTLASS (Simplifed Component Technique LASSO) permet d'introduire une borne sur la somme des valeurs absolues des coefficients, ces derniers devenant nul s'ils sont inf√©rieurs √† cette borne.

Soit X le jeu de donn√©es $X=(X_{1}, .., X_{p})^{T}$ et $R= corr(X)$ sa matrice de corr√©lation.
En faisant l'analyse en composante principale sur la matrice de corr√©lation, on obtient les composantes qui sont des combinaisons lin√©aires des $p$ variables mesur√©es, soit:

$$
Y_{k}= \alpha'_{k} X = \sum_{i=1}^{p} \alpha_{ki} X_{i}
$$

pour $(k=1, ...,p)$. On note ensuite la variance de l'axe principale $Y_{k}$ par $var(Y_{k})=\alpha'_{k}* R * \alpha_{k}$.

Le probl√®me de maximisation de l'ACP pour conserver la plus grande quantit√© d'information possible est donc donn√© par le param√®tre $\alpha$ qui maximise cette fonction:
$$
F(\alpha_1) = \alpha_1' R \alpha_1
$$
avec les contraintes suivantes:

\begin{gather*}
  \alpha'_k \alpha_k = 1 \qquad \forall k \\
  \text{cov(}\alpha_1'X, \alpha_2'X) = 0 \qquad\forall k  \geq 2 \text{ et } h \ne k
\end{gather*}

La m√©thode du LASSO appliqu√©e √† l'ACP (SCoTLASS) rajoute une troisi√®me contrainte sur les coefficients des variables sur les diff√©rents axes des composantes:

$$
\sum_{j=1}^{p} |a_{kj}| \leq t
$$

Dans cette m√©thode, il faut donc d√©finir un hyperparam√®tre ($t$) qui permettra de controler la r√©gularisation effectu√©e sur les coefficients de saturation. Il n'y a pas d'orientation particuli√®re pour le choix de $t$, mais on remarque que le choix de celui-ci peut √™tre d√©cortiqu√© de cette mani√®re:

1. pour $t \geq \sqrt{p}$, on a l'ACP.
2. pour $t \leq 1$, il n'existe pas de solution.
3. pour $t=1$, on a exactement une valeur non nulle de $a_{kj}$ pour chaque k.

Le point (2) est expliqu√© par la contrainte que le vecteur de coefficients de saturation doit √™tre norm√©. Si on veut effectuer une r√©gularisation sur les coefficients, il faut donc choisir une valeur de $t$ entre $1$ et $\sqrt{p}$. Une strat√©gie possible est d'essayer plusieurs valeurs diff√©rentes, mais cela a comme cons√©quence un co√ªt √©lev√© par rapport au temps de calcul.

Dans le choix du param√®tre $t$, il faut aussi consid√©rer le fait que plus la r√©gularisation est importante, plus les composantes seront facilement interpr√©tables, mais cela au co√ªt de perdre de la variabilit√©. Il faut donc trouver un √©quilibre entre la simplicit√© des composantes et la quantit√© d'information conserv√©e. Ce compromis d√©pend effectivement du jeu de donn√©es et √©galement du contexte.

En bref, la contrainte ajout√©e par la m√©thode SCoTLASS permet de r√©gulariser les valeurs des coefficients ce qui permet d'obtenir des coefficients de saturation exactement √©gals √† 0, ce qui facilite donc l'interpr√©tation des composantes.


### SPCA: Sparse Principal Component Analysis

Dans cette partie, on introduit une autre approche estimant les composantes principales tout en rendant les vecteurs de coefficients de saturation "sparse" et donc plus facile √† interpr√©ter. L'ACP peut √™tre r√©√©crite comme un probl√®me d'optimisation d'une r√©gression en imposant une p√©nalit√© quadratique: la p√©nalit√© du LASSO via l'Elastic net [@ESL].

Chaque composante principale peut √™tre d√©finit comme une combinaison lin√©aire des $p$ variables originales du jeu de donn√©es, donc les coefficients de saturation sur les composantes principales peuvent √™tre √©galement obtenus en faisant la r√©gression de la composante principale sur ces $p$ variables. Apr√®s l'application de l'ACP sur le jeu de donn√©e, on reconstruit les coefficients par une r√©gression ridge, ce qui veut dire que cette m√©thode d√©pend des r√©sultats obtenus avec l'ACP.

On note $Y_{i}$ la projection de la i√®me composante principale et $lambda$ une valeur positive. L'estimateur de ridge est donn√© par:
$$\hat{\beta}_{ridge}= arg min_{\beta} || Y_{i} - X\beta||^2 +\lambda ||\beta||^2 $$

Le deuxi√®me √©l√©ment dans la derni√®re √©quation, qui est contr√¥l√© par un hyperparam√®tre $lambda$, permet de contr√¥ler la r√©gularisation effectu√©e sur la r√©gression. Effectivement, plus il y a de coefficients non nuls dans la r√©gression, plus l'erreur est √©lev√©e. Cela aura donc pour effet de ne pas garder de variables inutiles dans le mod√®le.

Soit $\hat{v}=\frac{\hat{\beta}_{ridge}}{||\hat{\beta}_{ridge}||}$ et $\hat{v}=V_{i}$.
Cette p√©nalit√© de ridge n'en n'est pas vraiment une, elle sert simplement √† reconstruire les composantes.

Ensuite, la m√©thode rajoute une nouvelle p√©nalit√© $L_{1}$ √† l‚Äô√©quation pr√©c√©dente, la p√©nalit√© de Lasso, ce qui donne un nouveau probl√®me d'optimisation:

$$\hat{\beta}= arg min_{\beta} || Y_{i} - X\beta||^2 +\lambda ||\beta||^2+\lambda_{1} ||\beta||_{1} $$

telle que: $||\beta||_{1}= \sum_{j=1}^{p}|\beta_{j}|$ est la norme 1 de $\beta$. On appelle $\hat{V}_{i}= \frac{\hat{\beta}}{||\hat{\beta}||}$ l'approximation de $V_{i}$, et $X\hat{V}_{i}$ la i√®me composante principale approxim√©e.

Dans notre cas, on consid√®re les k premi√®res composantes principales. Soit $A_{p*k}=[\alpha_{1}, \alpha_{2}, ....,\alpha_{k}]$ et $B_{p*k}=[\beta_{1}, \beta_{2}, ....,\beta_{k}]$

Le probl√®me d'optimisation g√©n√©ralis√© obtenu est:
$$(\hat{A},\hat{B})=argmin \sum_{i=1}^{n} ||X_{i} -AB^{T}X_{i}||^2 + \sum_{j=1}^{k} \lambda||\beta_{j}||^2 + \sum_{j=1}^{k} \lambda_{1,j} ||\beta_{j}||_{1} $$
sous la contrainte $$A^T A = I_{k*k}$$

L'algorithme de cette m√©thode peut se r√©sumer comme suit:
1. On commence par $A = V[1:k]$ les coefficients de saturation des K premi√®res composantes principales.
2. Sachant $A= [\alpha_{1}, \alpha_{2}, ...,\alpha_{k}]$, On r√©sout le probl√®me pour $j=1, 2, ..., k$
$$\beta_j =arg min_ \beta(\alpha_j-\beta)^TX^TX(\alpha_j-\beta)+\lambda||\beta||^2 +\lambda_1,j||\beta||_1$$
3. Pour une matrice $B=[\beta_1, \beta_2, ..., \beta_k]$ fix√©e, on calcule la d√©composition de la matrice en valeur singuli√®re (SVD) de $X^TXB= UDV^T$, par la suite on met A √† jour, ce qui donne $A= UV^T$
4. On r√©p√®te les √©tapes 2-3 jusqu'√† la convergence.
5. On normalise V, $\hat V_j = \frac{\beta_j}{||\beta_j||}$

####Notes:
1. Les r√©sultats de cet algorithme ne change pas vraiment √† la variation de $\lambda$. En pratique, on choisi $\lambda$ un petit nombre positif pour √©viter les probl√®mes potentiels de colin√©arit√© dans X.

2. En principe, on peut choisir diff√©rentes combinaisons de $\{ \lambda_1,j\}$ pour avoir un bon choix des param√®tres et par l√† avoir un bon compromis entre la variance expliqu√©e et la sparsit√©.

## 4. Justification de la m√©thodologie

Pour justifier la m√©thodologie, il est possible de r√©aliser des simulations et voir comment performe l'analyse en composante principale parcimonieuse versus d'autres m√©thodes comme la m√©thode par rotation. Dans cet exemple, nous allons d√©finir $3$ structures de vecteurs et valeurs propres diff√©rentes que nous cherchons √† retrouver au travers de simulations. En effet, disons que nous avons une matrice et vecteurs propres $A$ et un vecteur de valeurs propres $I$, il est possible de retrouver la structure de corr√©lation de cette mani√®re:

$$
\Sigma = A DA^{-1}
$$
o√π D est une matrice diagonale construite avec les valeurs propres($I$). De cette mani√®re, on refait en quelque sorte le chemin inverse de la d√©composition en valeurs et vecteurs propres de la structure de corr√©lation (ou covariance). Une fois que la structure de corr√©lation est retrouv√©e, il est possible de simuler des lois normales multivari√©es √† partir de cette m√™me structure.

Les 3 structures diff√©rentes de vecteurs et valeurs propres sont d√©finis dans les tableaux 4, 5 et 6. La premi√®re est construite de mani√®re √† ce qu'il y aille 3 variables plus importantes et 3 variables moins importantes dans chacune des composantes. La troisi√®me structure est construite de mani√®re √† avoir des coefficients de saturation proches les uns des autres pour chacune des composantes. Pour finir, la deuxi√®me structure est une interm√©diaire entre les deux structures d√©crites plus t√¥t.

La figure 1 pr√©sente les matrices de corr√©lation pour les donn√©es simul√©es √† partir de ces $3$ diff√©rentes structures. Cela n'est pas si √©vident √† voir, mais si on regarde de pr√®s la figure 1 on remarque que la structure en blocks contient des blocks de variables tr√®s corr√©l√©es (carr√© de couleur tr√®s uniforme), alors que la structure uniforme contient des variables avec des corr√©lations plus uniform√©ment distribu√©es (carr√© de couleur moins uniforme).

```{r structure_blocks}
# Structure en blocks
vectors_blocks <- matrix(c(0.096, 0.082, 0.08, 0.594, 0.584, 0.533, -0.537, -0.565, -0.608, 0.085, 0.096, 0.074, 0.759, -0.599, -0.119, -0.074, -0.114, 0.18, -0.12, 0.231, -0.119, -0.308, -0.418, 0.805, 0.335, 0.511, -0.771, 0.069, 0.052, -0.157, -0.021, -0.013, 0.016, 0.731, -0.678, -0.069), nrow = 6, ncol = 6)
valeurs_blocks <- c(1.8367, 1.64, 0.751, 0.659, 0.607, 0.506)
data_structure_blocks <- data.table(
  variable = paste0("x", seq(1,6)),
  PC1 = vectors_blocks[,1],
  PC2 = vectors_blocks[,2],
  PC3 = vectors_blocks[,3],
  PC4 = vectors_blocks[,4],
  PC5 = vectors_blocks[,5],
  PC6 = vectors_blocks[,6]
)
knitr::kable(data_structure_blocks, format = "latex", booktabs = T, row.names = FALSE, align = rep("c", 6), caption = "Coefficients de saturation th√©oriques de la structure en blocks.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r structure_int}
# Structure intermediaire
vectors_intermediaires <- matrix(c(0.224, 0.253, 0.227, 0.553, 0.521, 0.507, -0.509, -0.519, -0.553, 0.249, 0.254, 0.199, 0.604, -0.361, -0.246, -0.249, -0.258, 0.561, 0.297, -0.644, 0.377, -0.052, 0.451, -0.384, -0.327, -0.341, 0.608, 0.262, -0.509, 0.281, 0.361, -0.064, -0.267, 0.706, -0.367, -0.402), nrow = 6, ncol = 6)
valeurs_intermediaires <- c(1.795, 1.674, 0.796, 0.618, 0.608, 0.510)
data_structure_intermediaire <- data.table(
  variable = paste0("x", seq(1,6)),
  PC1 = vectors_intermediaires[,1],
  PC2 = vectors_intermediaires[,2],
  PC3 = vectors_intermediaires[,3],
  PC4 = vectors_intermediaires[,4],
  PC5 = vectors_intermediaires[,5],
  PC6 = vectors_intermediaires[,6]
)
knitr::kable(data_structure_intermediaire, format = "latex", booktabs = T, row.names = FALSE, align = rep("c", 6), caption = "Coefficients de saturation th√©oriques de la structure interm√©diaire.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r structure_platte}
# Structure platte
vectors_plattes <- matrix(c(-0.455, -0.439, -0.415, 0.434, 0.301, 0.385, 0.336, 0.370, 0.422, 0.458, 0.435, 0.416, -0.087, -0.212, 0.378, 0.040, -0.697, 0.563, 0.741, -0.630, -0.110, -0.136, 0.114, 0.104, -0.328, -0.445, 0.697, -0.167, 0.356, -0.234, 0.125, -0.175, 0.099, 0.744, -0.306, -0.545), nrow = 6, ncol = 6)
valeurs_plattes <- c(1.841, 1.709, 0.801, 0.649, 0.520, 0.480)
data_structure_plattes <- data.table(
  variable = paste0("x", seq(1,6)),
  PC1 = vectors_plattes[,1],
  PC2 = vectors_plattes[,2],
  PC3 = vectors_plattes[,3],
  PC4 = vectors_plattes[,4],
  PC5 = vectors_plattes[,5],
  PC6 = vectors_plattes[,6]
)
knitr::kable(data_structure_plattes, format = "latex", booktabs = T, row.names = FALSE, align = rep("c", 6), caption = "Coefficients de saturation th√©oriques de la structure uniforme.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r simulations}

# Load packages -----------------------------------------------------------

library(propagate)
library(psych)
library(elasticnet)

# Simuler les donn√©es -----------------------------------------------------

cov_function <- function(vecteurs_propres, valeurs_propres, cov = TRUE){
  
  n <- nrow(vecteurs_propres)
  S = vecteurs_propres
  M = matrix(rep(0, n^2), ncol = n, nrow = n)
  diag(M) <- valeurs_propres
  
  cor = round(S %*% M %*% solve(S), 6)
  
  for (j in 1:ncol(cor)) {
    for (i in 1:j) {
      if (cor[i, j] != diag(cor)[j]){
        cor[i, j] = cor[j, i]
      }
    }
  }
  
  if (cov){
    cov = round(propagate::cor2cov(C = cor, var = valeurs_propres), 6)
  } else {
    round(cor, 6)
  }
}

cov_blocks <- cov_function(vecteurs_propres = vectors_blocks, valeurs_propres = valeurs_blocks)
cov_intermediaire <- cov_function(vecteurs_propres = vectors_intermediaires, valeurs_propres = valeurs_intermediaires)
cov_platte <- cov_function(vecteurs_propres = vectors_plattes, valeurs_propres = valeurs_plattes)

data_blocks <- rmvnorm(100000, mean = rep(0, 6), sigma = cov_blocks)
data_intermediaire <- rmvnorm(100000, mean = rep(0, 6), sigma = cov_intermediaire)
data_platte <- rmvnorm(100000, mean = rep(0, 6), sigma = cov_platte)


# Appliquer les diff√©rentes m√©thodes --------------------------------------

acp_blocks <- prcomp(cor(data_blocks), center = FALSE)
rpca_blocks <- principal(cor(data_blocks), nfactors = 6, rotate = "varimax")
spca_blocks <- spca(cor(data_blocks), K = 6, para = rep(3,6), sparse = "varnum", type = "Gram")

acp_intermediaire <- prcomp(cor(data_intermediaire), center = FALSE)
rpca_intermediaire <- principal(cor(data_intermediaire), nfactors = 6, rotate = "varimax")
spca_intermediaire <- spca(cor(data_intermediaire), K = 6, para = rep(0.01, 6), sparse = "penalty", type = "Gram")

acp_platte <- prcomp(cor(data_platte), center = FALSE)
rpca_platte <- principal(cor(data_platte), nfactors = 6, rotate = "varimax")
spca_platte <- spca(cor(data_platte), K = 6, para = rep(0.01, 6), sparse = "penalty", type = "Gram")

# Analyser les performances -----------------------------------------------

dist_loadings <- function(x,y){
  sqrt(1 - (t(x) %*% y)^2)
}

# Structure blocks
# sapply(1:6, function(x) {
#   dist_loadings(acp_blocks$rotation[,x], vectors_blocks[,x])
# })
dist_rpca_blocks <- sapply(1:6, function(x) {
  dist_loadings(rpca_blocks$loadings[,x], vectors_blocks[,x])
})
dist_spca_blocks <- sapply(1:6, function(x) {
  dist_loadings(spca_blocks$loadings[,x], vectors_blocks[,x])
})

# Structure intermediaire
# sapply(1:6, function(x) {
#   dist_loadings(acp_intermediaire$rotation[,x], vectors_intermediaires[,x])
# })
dist_rpca_int <- sapply(1:6, function(x) {
  dist_loadings(rpca_intermediaire$loadings[,x], vectors_intermediaires[,x])
})
dist_spca_int <- sapply(1:6, function(x) {
  dist_loadings(spca_intermediaire$loadings[,x], vectors_intermediaires[,x])
})

# Structure platte
# sapply(1:6, function(x) {
#   dist_loadings(acp_plattes$rotation[,x], vectors_plattes[,x])
# })
dist_rpca_platte <- sapply(1:6, function(x) {
  dist_loadings(rpca_platte$loadings[,x], vectors_plattes[,x])
})
dist_spca_platte <- sapply(1:6, function(x) {
  dist_loadings(spca_platte$loadings[,x], vectors_plattes[,x])
})
```

```{r cor_plot, echo = FALSE, fig.align = 'center', fig.cap = "Repr√©sentations des diff√©rentes matrices de corr√©lation pour les donn√©es simul√©es √† partir des 3 diff√©rentes structures."}
library(reshape2)
library(ggplot2)
library(gridExtra)
library(dplyr)

colnames(data_blocks) <- paste0("Var", seq(1:6))
colnames(data_intermediaire) <- paste0("Var", seq(1:6))
colnames(data_platte) <- paste0("Var", seq(1:6))

cor_blocks <- round(cor(data_blocks), 2)
cor_intermediaires <- round(cor(data_intermediaire), 2)
cor_platte <- round(cor(data_platte), 2)

melted_cor_blocks <- data.table(melt(cor_blocks))[, structure := "Block"]
melted_cor_int <- data.table(melt(cor_intermediaires))[, structure := "Interm√©diaire"]
melted_cor_platte <- data.table(melt(cor_platte))[, structure := "Uniforme"]

rbindlist(list(melted_cor_blocks, melted_cor_int, melted_cor_platte), use.names = TRUE, fill = FALSE) %>% 
  ggplot(aes(x = Var1, y = Var2, fill = value)) + 
    geom_tile() +
    facet_grid(. ~ structure) +
    scale_fill_continuous("Corr√©lation") +
    theme(legend.position = "bottom", axis.title.x = element_blank(), axis.title.y = element_blank())
```

√Ä partir des donn√©es simul√©es, il est possible d'appliquer l'ACP suivi de la rotation varimax ainsi que l'ACP parcimonieuse et voir quelle des deux m√©thodes permet le mieux de retrouver les structures d√©finit initialement. Les tableaux 7, 8 et 9 montrent les distances entre les vecteurs de coefficients de saturation trouv√©s par les deux m√©thodes avec les "vrais" vecteurs de coefficients d√©finit dans les 3 structures diff√©rentes. 

Dans les tableaux 7, 8 et 9, on remarque que la m√©thode d'analyse en composante principale parcimonieuse (SPCA) retrouve des coefficients de corr√©lation qui sont plus pr√®s de ceux d√©finis au d√©part, et ce, pour les $3$ types de structure.

```{r distance_blocks}
# Structure en blocks
data_structure_blocks <- data.table(
  methode = c("RPCA", "SPCA"),
  PC1 = c(dist_rpca_blocks[1], dist_spca_blocks[1]),
  PC2 = c(dist_rpca_blocks[2], dist_spca_blocks[2]),
  PC3 = c(dist_rpca_blocks[3], dist_spca_blocks[3]),
  PC4 = c(dist_rpca_blocks[4], dist_spca_blocks[4]),
  PC5 = c(dist_rpca_blocks[5], dist_spca_blocks[5]),
  PC6 = c(dist_rpca_blocks[6], dist_spca_blocks[6])
)
knitr::kable(data_structure_blocks, format = "latex", booktabs = T, row.names = FALSE, col.names = c("M√©thode", paste0("PC", seq(1,6))), align = rep("c", 7), caption = "Distances entre les vecteurs de coefficients de saturation reconstruits d'apr√®s les 2 m√©thodes avec les vecteurs d√©finis dans la structure en blocks.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r distance_int}
# Structure intermediaire
data_structure_intermediaire <- data.table(
  methode = c("RPCA", "SPCA"),
  PC1 = c(dist_rpca_int[1], dist_spca_int[1]),
  PC2 = c(dist_rpca_int[2], dist_spca_int[2]),
  PC3 = c(dist_rpca_int[3], dist_spca_int[3]),
  PC4 = c(dist_rpca_int[4], dist_spca_int[4]),
  PC5 = c(dist_rpca_int[5], dist_spca_int[5]),
  PC6 = c(dist_rpca_int[6], dist_spca_int[6])
)
knitr::kable(data_structure_intermediaire, format = "latex", booktabs = T, row.names = FALSE, col.names = c("M√©thode", paste0("PC", seq(1,6))), align = rep("c", 7), caption = "Distances entre les vecteurs de coefficients de saturation reconstruits d'apr√®s les 2 m√©thodes avec les vecteurs d√©finis dans la structure en interm√©diaire.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r distance_platte}
# Structure platte
data_structure_platte <- data.table(
  methode = c("RPCA", "SPCA"),
  PC1 = c(dist_rpca_platte[1], dist_spca_platte[1]),
  PC2 = c(dist_rpca_platte[2], dist_spca_platte[2]),
  PC3 = c(dist_rpca_platte[3], dist_spca_platte[3]),
  PC4 = c(dist_rpca_platte[4], dist_spca_platte[4]),
  PC5 = c(dist_rpca_platte[5], dist_spca_platte[5]),
  PC6 = c(dist_rpca_platte[6], dist_spca_platte[6])
)
knitr::kable(data_structure_platte, format = "latex", booktabs = T, row.names = FALSE, col.names = c("M√©thode", paste0("PC", seq(1,6))), align = rep("c", 7), caption = "Distances entre les vecteurs de coefficients de saturation reconstruits d'apr√®s les 2 m√©thodes avec les vecteurs d√©finis dans la structure uniforme.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

## 5. Application de la m√©thodologie

Cette partie a comme but de revenir au jeu de donn√©es pr√©sent√© dans la section "Exemple de motivation" et voir comment se comporte les m√©thodes introduites dans ce document. On se rappelle que l'ACP classique appliqu√©e sur le jeu de donn√©es pitprops permettait de d√©finir des composantes principales qui √©taient difficiles √† interpr√©ter. Pour palier √† ce probl√®me, il √©tait possible de faire une rotation de ces composantes. Toutefois, cela amenait d'autres probl√®mes comme la perte d'information dans les premi√®res composantes principales ainsi que d'autres probl√®mes d'interpr√©tabilit√©.

Dans cette section, nous allons donc comparer les r√©sultats obtenus par les trois m√©thodes suivantes:

1. L'ACP classique (PCA)
2. L'ACP suivi d'une rotation varimax (RPCA)
3. L'analyse en composante principale parcimonieuse (SPCA)

La premi√®re m√©thode (ACP) est impl√©ment√©e dans plusieurs librairies *R*, mais pour l'exemple nous avions utilis√© la fonction *prcomp* de la librairie **stats**. Pour la deuxi√®me m√©thode (RPCA), nous avons utilis√© la fonction *principal* de la librairie **psych**. Pour la derni√®re m√©thode, nous avons utilis√© la librairie **elasticnet** et la fonction *spca*.

Les hyperparam√®tres de la m√©thode SPCA ont √©t√© d√©finis dans le but d'obtenir une variance expliqu√©e similaire √† celle de l'ACP pour les $6$ premi√®res composantes principales. Voici les param√®tres:

\begin{gather*}
  \lambda=0.000001 \\
  \lambda_1= [0.06, 0.16, 0.1, 0.5, 0.5, 0.5]
\end{gather*}

On commence par d√©finir les hyperparam√®tres d√©finits ci-dessus et on applique la fonction sur le jeu de donn√©es pitpros.

```{r spca, echo = TRUE}
library(elasticnet)
data("pitprops")

lambda <- 0.000001
lambda_1 <- c(0.06, 0.16, 0.1, 0.5, 0.5, 0.5)

spca_pitprops <- spca(pitprops, K = 6, type = "Gram", sparse = "penalty",
                      para = lambda_1, lambda = lambda)
```

En appliquant la m√©thode SPCA sur le jeu de donn√©es, on obtient des coefficients de saturation qui sont en fonction d'un plus petit nombre de variables. Ces diff√©rents vecteurs de coefficients de saturation sont pr√©sent√©s dans le tableau 4. 

```{r application}
data_spca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(spca_pitprops$loadings[, 1]), spca_pitprops$pev[1], sum(spca_pitprops$pev[1:1])), 3),
  PC2 = round(c(as.numeric(spca_pitprops$loadings[, 2]), spca_pitprops$pev[2], sum(spca_pitprops$pev[1:2])), 3),
  PC3 = round(c(as.numeric(spca_pitprops$loadings[, 3]), spca_pitprops$pev[3], sum(spca_pitprops$pev[1:3])), 3),
  PC4 = round(c(as.numeric(spca_pitprops$loadings[, 4]), spca_pitprops$pev[4], sum(spca_pitprops$pev[1:4])), 3),
  PC5 = round(c(as.numeric(spca_pitprops$loadings[, 5]), spca_pitprops$pev[5], sum(spca_pitprops$pev[1:5])), 3),
  PC6 = round(c(as.numeric(spca_pitprops$loadings[, 6]), spca_pitprops$pev[6], sum(spca_pitprops$pev[1:6])), 3)
)

knitr::kable(data_spca, format = "latex", booktabs = T, row.names = FALSE, col.names = c("", paste0("PC", seq(1,6))), align = rep("c", 6), caption = "Coefficients de saturation de l'analyse en composante principale parcimonieuse (SPCA) effectu√©e sur la matrice de corr√©lation du jeu de donn√©es pitprops.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

En analysant le tableau 10, on remarque que 7 variables sur 13 sont non nulles pour la premi√®re composante principale. Pour la deuxi√®me et la troisi√®me, 4 sur 13 sont non nulles. Cela permet donc de faciliter l'interpr√©tation des composantes, car on peut imm√©diatement √©carter les coefficients nuls, comparativement √† la m√©thode de la rotation, qui ne permettait pas cela. Ainsi, on peut conclure que la m√©thode d'analyse en composante principale parcimonieuse donne les r√©sultats les plus faciles √† interpr√©ter, suivi dans l'ordre par la m√©thode de la rotation et de l'ACP classique. Il est √©galement possible de voir, pour chacune des composantes principales, si le nombre de coefficients nuls de la m√©thode SPCA sont enlign√©s avec le nombre coefficients "faibles" (<= 0.2) des autres m√©thodes.

```{r graph_coeff, fig.align='center', fig.cap="Nombre de coefficients de saturation nuls ou faible (<=0.2) selon les diff√©rentes m√©thodes."}
acp_rotated <- principal(pitprops, 6, rotate = "varimax", eps = 1e-14)
acp <- prcomp(x = pitprops, center = FALSE)

data.table(
  PC = rep(as.character(seq(1:6)), 3),
  methode = c(rep("SPCA", 6), rep("RPCA", 6), rep("PCA", 6)),
  nb_nuls = c(sapply(1:6, function(x) sum(spca_pitprops$loadings[,x] == 0)), sapply(1:6, function(x) sum(abs(acp_rotated$loadings[,x]) <= 0.3)), sapply(1:6, function(x) sum(abs(acp$rotation[,x]) <= 0.3)))
) %>% 
  ggplot(aes(x = PC, y = nb_nuls, group = methode, color = methode)) +
    geom_point() +
    geom_line() +
    scale_x_discrete("Composante principale") +
    scale_y_continuous("Nombre de coefficents nuls ou plus petit que 0.2") +
    scale_color_discrete("M√©thode") +
    theme_classic()
```

On remarque √† la figure 2 que la r√©gularisation faite par la m√©thode SPCA est forte sur les composantes 4 √† 6. Cela est effectivement contr√¥l√© par l'hyperparam√®tre $\lambda_1$ d√©finit un peu plus t√¥t. Pour les premi√®res composantes, on voit qu'il y a un lien entre le nombres de coefficients nuls de la m√©thode SPCA et le nombre de coefficients faibles des autres m√©thodes. Cela veut donc dire que les coefficients qui ont √©t√© tir√© vers $0$ sont probablement des coefficients "peu importants" et sont ceux que nous aurions exclut intuitivement de notre interpr√©tation.

Comme mentionn√© pr√©c√©demment, l'am√©lioration de l'interpr√©tabilit√© se fait souvent au prix de perte de l'information, ou de la variabilit√© dans les composantes principales. En regardant de plus pr√®s les tableaux 2, 3 et 4, on remarque que la rotation varimax et la m√©thode SPCA ont le m√™me genre de pertes au niveau de la variabilit√©. Toutefois, √©tant donn√© que nous avons un gain au niveau de l'interpr√©tabilit√© avec la m√©thode d'analyse en composante principale parcimonieuse, il est possible de conclure que celle-ci constitue une am√©lioration par rapport aux autres m√©thodes comme la rotation.

## 6. Autres consid√©rations de la m√©thode

Dans ce document, nous avons d√©crits deux m√©thodes en lien avec l'analyse en composantes principales parcimonieuse, soit la m√©thode SCoTLASS et la m√©thode SPCA. Il est pertinent de mentionner qu'une autre m√©thode a √©t√© d√©velopp√©e, soit la m√©thode PMA [@Witten]. Cette derni√®re est bas√©e sur une d√©composition p√©nalis√©e de la matrice de covariance (ou corr√©lation). Il existe d'ailleurs une impl√©mentation de cette m√©thode en **R** qu'on peut retrouver dans la librarie *PMA*.

## 7. Bibliographie
