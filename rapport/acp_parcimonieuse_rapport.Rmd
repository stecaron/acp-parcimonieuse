---
title: "ACP Parcimonieuse"
author: "Stéphane Caron/Sofia Harrouch"
date: '2018-04-04'
output:
  pdf_document:
    toc: yes
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## 1. Introduction

Les méthodes statistiques de réduction de la dimensionnalité ont généralement comme objectif de réduire la dimension d'un jeu de données dans le but de simplifier l'interprétation des données, de permettre la visualisation des données ou même d'améliorer la performance de certaines méthodes appliquées sur ces données réduites. En termes simples, réduire la dimensionnalité revient à réduire le nombre de variables (p) mesurées.

L'analyse en composantes principales est une méthode classique de réduction de la dimensionnalité. Cette méthode permet de créer des combinaisons linéaires des différentes variables du jeu de données tout en conservant le plus de variabilité possible. Chacune des nouvelles composantes principales créées possède un vecteur de coefficients de saturation (loadings) de dimension $p\text{ x }1$, correspondant en quelque sorte à l'importance attribuée à chacune des différentes variables originales du jeu de données. Il est donc possible d'interpréter ces coefficients de saturation et d'obtenir une interprétation plus généralisée de certaines composantes principales calculées.

Cependant, cette interprétation peut se révéler assez complexe dans le cas où une composante principale est expliquée (coefficients de saturation élevés) par plusieurs variables originales du jeu de données. De plus, il peut être difficile de définir à partir de quelle valeur exactement un coefficient de saturation est considéré comme étant "non important" pour une composante principale. Pour palier à ce problème d'interprétation, il existe différentes méthodes connues. Par exemple, les rotations [@Jolliffe1989] cherchent à simplifier l'interprétation des composantes principales. Il pourrait également être possible d'écarter les coefficients de saturation inférieurs à une certaine valeur ou simplement de restreindre les valeurs possibles que ces coefficients peuvent prendre (ex: -1, 0 ou 1). Ces méthodes sont des exemples de stratégie permettant de faciliter l'interprétation des composantes principales, mais elles ont toutes certains désavantages.

La méthodologie introduite dans le présent document est en quelque sorte une alternative à ces méthodes. En bref, elle consiste à ajouter certaines contraintes au modèle d'analyse en composantes principales qui auront comme objectif d'améliorer l'interprétabilité des composantes calculées. Cela permettra notamment d'obtenir des coefficients de saturation exactement égales à zéro. On pourrait donc dire que cette méthode permet de combiner l'aspect de réduction de la dimensionnalité apportée par l'ACP et l'aspect de simplification de l'interprétabilité apporté par les exemples décrits plus haut. 

La section 2 fera l'illustration du genre de problème qu'on peut éprouver avec l'analyse en composante principale et les rotations en terme d'interprétatibilité. La section 3 a comme objectif de décrire la méthodologie. Dans la section 4, nous verrons plus en détails la justification théorique et les résultats de simulation de la méthodologie. La section 5 permettra d'illustrer avec un exemple complet les résultats de la méthodologie.


## 2. Exemple de motivation

Pour illustrer la motivation derrière la méthodologie, supposons qu'on cherche à simplifier un jeu de données provenant d'un échantillon de 180 coupes de bois de pin afin d'avoir une meilleure compréhension des différentes mesures (variables) impliquées. Les différentes variables du jeu de données en question sont présentées dans le tableau 1. À partir de la matrice de corrélation, il est possible de commencer par faire l'analyse en composante principale et analyser les différentes composantes calculées.

```{r import_data, echo = TRUE}
library(elasticnet)

# Ce jeu de données correspond à la matrice de corrélation
data(pitprops)
```


```{r dataset}
library(data.table)

data_description <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops)))),
  description = c("Diamètre dans le haut de l'arbre (en pouces)", "Longeur (en pouces)", "Humidité (% poids sec)", "Gravité au moment du test", "Nombre d'anneaux dans le haut de l'arbre", "Nombre d'anneaux dans le bas de l'arbre", "Branche principale (en pouces)", "Distance du bout de la branche principale au haut de l'arbre", "Nombre de spires", "Longueur de l'hélice transparente du haut (en pouces)", "Nombre moyen de nœuds par verticille", "Diamètre moyen des noeuds (en pouces)")
)
knitr::kable(x = data_description, col.names = c("Variables", "Description"), row.names = FALSE, align = c("c", "l"), caption = "Présentation des différentes variables du jeu de données pitprops.")
```

Comme mentionné dans l'introduction, l'ACP consiste essentiellement à trouver des combinaisons linéaires des variables originales du jeu données (disons la matrice $X$) tout en maximisant la variance. En termes plus théoriques, la première composante principale est calculée en maximisant la fonction

$$
F(\alpha_1) = \alpha_1' \Sigma \alpha_1
$$
avec la contrainte que $\alpha_1'\alpha_1=1$. La matrice $\Sigma$ correspond à la matrice de covariance ou à la matrice de corrélation (dépend de la situation).

Le vecteur $\alpha_1$ correspond au vecteur de coefficients de saturation de la première composante principale. On refait la même chose pour la deuxième composante principale en ajoutant la contrainte que:

$$
cov(\alpha_1'X, \alpha_2'X) = 0
$$
Il est également possible de démontrer que si on fait la décomposition en valeurs singulières de la matrice de corrélation (ou covariance), on trouve que les vecteurs $\alpha_1, ..., \alpha_p$ correspondent aux vecteurs propres normés de la matrice $\Sigma$ alors que la variance de chacune des composantes principales correspond aux $p$ valeurs propres de la même matrice $\Sigma$. Ainsi, après avoir fait la décomposition en valeurs et vecteurs propres de la matrice de corrélation, il est possible d'analyser essentiellement deux choses:

1. L'interprétabilité de chacune des composantes principales
2. L'information conservée à chacune des composantes principales

La première peut être analysée en tentant d'interpréter les coefficients de saturation (vecteurs propres). Plus il y a de coefficients similaires, plus la composante est difficile à interpréter. À l'extrême, le cas le plus simple serait le cas où seulement un seul coefficient ne serait pas égale à 0. Dans ce cas-ci, la composante serait effectivement facile à interpréter, mais il y aurait probablement beaucoup de perte d'information (peu de variance expliquée), ce qui n'est pas nécessairement désiré. L'information conservée à chacune des composantes peut quant à elle être quantifiée avec la variance expliquée par la composante principale.

Le tableau 2 montre les résultats des 6 premières composantes principales calculées par l'ACP. On garde seulement les 6 premières composantes étant donné qu'elles expliquent plus de 87% de la variabilité totale du jeu de données.

```{r acp, echo = TRUE}
library(stats)

# Faire l'ACP classique sur la matrice de corrélation
acp <- prcomp(x = pitprops, center = FALSE)
```



```{r acp_results}
options(scipen = 999)
library(dplyr)
library(kableExtra)

data_pca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(acp$rotation[, 1]), 100*acp$sdev[1]/sum(acp$sdev), 100*cumsum(acp$sdev)[1]/sum(acp$sdev)), 3),
  PC2 = round(c(as.numeric(acp$rotation[, 2]), 100*acp$sdev[2]/sum(acp$sdev), 100*cumsum(acp$sdev)[2]/sum(acp$sdev)), 3),
  PC3 = round(c(as.numeric(acp$rotation[, 3]), 100*acp$sdev[3]/sum(acp$sdev), 100*cumsum(acp$sdev)[3]/sum(acp$sdev)), 3),
  PC4 = round(c(as.numeric(acp$rotation[, 4]), 100*acp$sdev[4]/sum(acp$sdev), 100*cumsum(acp$sdev)[4]/sum(acp$sdev)), 3),
  PC5 = round(c(as.numeric(acp$rotation[, 5]), 100*acp$sdev[5]/sum(acp$sdev), 100*cumsum(acp$sdev)[5]/sum(acp$sdev)), 3),
  PC6 = round(c(as.numeric(acp$rotation[, 6]), 100*acp$sdev[6]/sum(acp$sdev), 100*cumsum(acp$sdev)[6]/sum(acp$sdev)), 3)
)
knitr::kable(data_pca, format = "latex", booktabs = T, row.names = FALSE, col.names = c("", paste0("PC", seq(1:6))), align = rep("c", 6), caption = "Coefficients de saturation de l'analyse en composante principale effectuée sur la matrice de corrélation du jeu de données pitprops.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

Dans le tableau 2, on remarque que les premières composantes principales ont beaucoup de coefficients qui se ressemblent, ce qui rend difficile l'interprétation de celles-ci. Pour palier à ce problème, nous pouvons effectuer une rotation de ces composantes principales. Une rotation classique dans ce genre de situation serait la rotation varimax. Cette rotation est de type orthogonale, c'est donc dire que le système de coordonnées actuel ne subit seulement qu'une rotation. La rotation est faite dans le but de rapprocher le plus possible les coefficients de saturation vers 0 ou 1. Le tableau 3 montre les résultats obtenus après avoir effectué la rotation varimax.

```{r rotation, echo = TRUE}
library(psych)

# Faire l'ACP, mais en faisant une rotation varimax des CP
acp_rotated <- principal(pitprops, 6, rotate = "varimax", eps = 1e-14)
```


```{r rotation_results}
data_rpca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(acp_rotated$loadings[, 1]), 0.28, 0.28), 3),
  PC2 = round(c(as.numeric(acp_rotated$loadings[, 2]), 0.15, 0.43), 3),
  PC3 = round(c(as.numeric(acp_rotated$loadings[, 3]), 0.12, 0.56), 3),
  PC4 = round(c(as.numeric(acp_rotated$loadings[, 4]), 0.12, 0.67), 3),
  PC5 = round(c(as.numeric(acp_rotated$loadings[, 5]), 0.11, 0.78), 3),
  PC6 = round(c(as.numeric(acp_rotated$loadings[, 6]), 0.09, 0.87), 3)
)
knitr::kable(data_rpca, format = "latex", booktabs = T, row.names = FALSE, col.names = c("", paste0("PC", c(1,2,3,6,5,4))), align = rep("c", 6), caption = "Coefficients de saturation de l'analyse en composante principale effectuée sur la matrice de corrélation du jeu de données pitprops après avoir effectué une rotation orthogonale 'varimax'.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

En analysant de plus près le tableau 3, on remarque que la rotation effectuée a permis d'améliorer légèrement l'interprétabilité des premières composantes. Désormais, on remarque que les variables x1 et x2 se démarquent davantage des autres dans la première composante, même chose pour x3 et x4 dans la deuxième composante. Bien que la rotation permet d'améliorer l'interprétabilité, on remarque qu'on perd de la variabilité dans les premières composantes après la rotation. Dans l'ACP classique, les 3 premières composantes expliquaient environ 65% de la variabilité alors que dans le cas de l'ACP avec rotation, les 3 mêmes composantes expliquent environ 56% de la variabilité. De plus, on remarque que ce ne sont plus nécessairement les mêmes composantes qui expliquent successivement le maximum de variabilité. Par exemple, la 4ème composante principale dans l'ACP classique est celle qui explique le moins de variabilité après la rotation. Ces constats sont en quelque sorte les inconvénients pouvant être rattachés aux rotations et sont également la motivation derrière l'ACP parcimonieuse.

## 3. Description de la méthodologie

Comme mentionné précédemment, l'ACP souffre parfois du problème d'interprétation des axes, ou des vecteurs de coefficients de saturation. Pour palier à ce problème, il est possible d'avoir recours à une méthode spécifique, soit l'ACP parcimonieuse. Cette méthode permet de trouver des axes "éparses" qui sont expliqués par un petit nombre des variables seulement. On pourrait donc dire que la méthode fait en quelque sorte une régularisation et ignore l'effet de certaines variables sur les axes principales. Une connaissance sur les méthodes de régularisation peut donc être fort utile pour bien comprendre la mécanique derrière les méthodes d'ACP parcimonieuse. Ainsi, nous avons ajouté en annexe une section faisait une brève introduction sur les fondements derrière les méthodes de régularisation. 

Dans cette section, on expliquera les différentes méthodes proposées pour l'estimation de vecteurs de coefficients de saturation. La première méthode est basée sur la propriété d'obtention d'une variance maximale des composantes principales (SCoTLASS) alors que la la deuxième méthode est construite en se basant sur la propriété de l'erreur de reconstruction (SPCA).

### Méthode 1: SCoTLASS

Cette technique [@Jolliffe2003], emprunte l'idée du LASSO (Least Absolute Shrinkage and Selection Operation) introduite par Tibshirani (1996). Le LASSO est généralement appliqué en régression multiple quand le nombre de variables est élevé en faisant une régularisation sur celles-ci. Dans le problème de l'interprétabilité de l'ACP, cette méthode peut se révéler fort utile pour garder uniquement les variables importantes et ainsi faciliter l'interprétation. La méthode SCoTLASS (Simplifed Component Technique LASSO) permet d'introduire une borne sur la somme des valeurs absolues des coefficients, ces derniers devenant nul s'ils sont inférieurs à cette borne.

Soit X le jeu de données $X=(X_{1}, .., X_{p})^{T}$ et $R= corr(X)$ sa matrice de corrélation.
En faisant l'analyse en composante principale sur la matrice de corrélation, on obtient les composantes qui sont des combinaisons linéaires des $p$ variables mesurées, soit:

$$
Y_{k}= \alpha'_{k} X = \sum_{i=1}^{p} \alpha_{ki} X_{i}
$$

pour $(k=1, ...,p)$. On note ensuite la variance de l'axe principale $Y_{k}$ par $var(Y_{k})=\alpha'_{k}* R * \alpha_{k}$.

Le problème de maximisation de l'ACP pour conserver la plus grande quantité d'information possible est donc donné par le paramètre $\alpha$ qui maximise cette fonction:
$$
F(\alpha_1) = \alpha_k' R \alpha_k
$$
avec les contraintes suivantes:

\begin{gather*}
  \alpha'_k \alpha_k = 1 \qquad \forall k \\
  \text{cov(}\alpha_k'X, \alpha_h'X) = 0 \qquad\forall h  \geq 2 \text{ et } h \ne k
\end{gather*}

La méthode du LASSO appliquée à l'ACP (SCoTLASS) rajoute une troisième contrainte sur les coefficients des variables sur les différents axes des composantes:

$$
\sum_{j=1}^{p} |a_{kj}| \leq t
$$

Dans cette méthode, il faut donc définir un hyperparamètre ($t$) qui permettra de controler la régularisation effectuée sur les coefficients de saturation. Il n'y a pas d'orientation particulière pour le choix de $t$, mais on remarque que le choix de celui-ci peut être décortiqué de cette manière:

1. pour $t \geq \sqrt{p}$, on a l'ACP.
2. pour $t \leq 1$, il n'existe pas de solution.
3. pour $t=1$, on a exactement une valeur non nulle de $a_{kj}$ pour chaque k.

Le point (2) est expliqué par la contrainte que le vecteur de coefficients de saturation doit être normé. Si on veut effectuer une régularisation sur les coefficients, il faut donc choisir une valeur de $t$ entre $1$ et $\sqrt{p}$. Une stratégie possible est d'essayer plusieurs valeurs différentes, mais cela a comme conséquence un coût élevé par rapport au temps de calcul.

Dans le choix du paramètre $t$, il faut aussi considérer le fait que plus la régularisation est importante, plus les composantes seront facilement interprétables, mais cela au coût de perdre de la variabilité. Il faut donc trouver un équilibre entre la simplicité des composantes et la quantité d'information conservée. Ce compromis dépend effectivement du jeu de données et également du contexte.

En bref, la contrainte ajoutée par la méthode SCoTLASS permet de régulariser les valeurs des coefficients ce qui permet d'obtenir des coefficients de saturation exactement égals à 0, ce qui facilite donc l'interprétation des composantes.


### SPCA: Sparse Principal Component Analysis

Dans cette partie, on montre une deuxième approche pour l'obtention d'une ACP modifiée et *sparse*. Pour cette méthode, on peut faire l'analogie entre l'ACP et la regression de Ridge. Tout d'abord, on montrera que les composantes principales d'une ACP peuvent être écrites comme un problème d'optimisation d'une regression de Ridge. Finalement, on ajoute la pénalité du Lasso afin d'obtenir un problème d'optimisation sous la forme d'une regression sous la pénalité d'Elastic Net ce qui va nous permettre d'obtenir des composantes principales parcimonieuses.

#### Approche simple de regression pour l'ACP

On considère $Y_k =\alpha'_k X$ la kième composante principale obtenu à partir d'une ACP, qui est une combinaison linéaire des $p$ variables intiales. En fait, ces coefficients de saturation $\alpha_k$ peuvent aussi être obtenu en faisant une regression multiple de la composante principale sur les $p$ variables initiales ($Y_k = X\beta + \epsilon$). Ensuite, on peut étendre cette regression à une regression  de Ridge en ajoutant la pénalité $L_2$ dans le but de manipuler toute sorte de données et obtenir une solution unique comme l'ACP.

Pour résumer, cette pénalité nous permet juste de reconstruire les composantes principales et non pas de les "pénaliser". Enfin, on rajoute la pénalité du Lasso ($L_1$) pour pénaliser les composantes et de les rendre parcimonieuses. Le problème d'optimisation devient:

$$\hat\beta = argmin_\beta ||Y_k -X\beta||+\lambda||\beta||^2+\lambda_1||\beta||_1 \quad(1.1)$$

et la kième composante principale approximée est $X \hat V_k= X \frac{\hat\beta}{||\hat \beta||}$.

Cette technique dépend essentiellement du résultat de l'ACP, car on applique l'ACP et on utilise l'équation (1.1) pour trouver une autre apprixmation appropriée des composantes principales pour qu'elles soient parcimonieuses. Dans la prochaine section, nous verrons comment il est possible d'utiliser l'ACP uniquement comme point de départ et ensuite d'itérativement améliorer les estimations des coefficients parcimonieux.

#### Approche complexe de regression pour l'ACP

Soient $\mathbf{X}$ la matrice d'observations, $x_i$ la ième ligne de cette matrice, $A_{p\times k}=[\alpha_1,\alpha_2,...,\alpha_k]$ les k premières composantes principales et $B_{p\times k}=[\beta_1,\beta_2,....,\beta_k]$ les k premières composantes principales estimées de tel sorte que les composantes principales soient parcimonieuses.

Le théorème 1 suivant montre que le problème de l'ACP peut se transformer en un problème de regression.

**Théorème 1**
Pour tout $\lambda>0$:
$$(\hat A, \hat B)= argmin_{A,B} \sum_{i=1}^n||x_i -AB^Tx_i||^2 +\lambda \sum_{j=1}^{k}||\beta_j||^2$$
Sous la contrainte $$A^TA = I_{k*k}$$

Alors $\hat \beta_j \infty V_j$ pour $j=1,2,...,k$.

Ce théorème nous permet de bien voir la transformation du problème d'optimisation.

En fait,
$$\sum_{i=1}^n||x_i -AB^Tx_i||^2 =||X -XBA^T||^2$$

$$= ||XA_{ \perp}||^2+ ||XA-XB||^2$$

Comme $A$ est orthonormale, $A_{\perp}$ est une matrice quelconque orthonormale telle que $[A;A_{ \perp}]$ est $p\times p$ orthonormal.

Pour A fixé, le problème de minimisation devient:

$$(A, \hat B)=argmin_B C_{\lambda}(A,\hat B)= argmin_{A,B} \sum_{j=1}^{k}||X\alpha_j - X\beta_j||^2 +\lambda \sum_{j=1}^{k}||\beta_j||^2$$

Ce qui est équivalent à resoudre k regressions de Ridge indépendantes. Cela nous donne $\hat\beta=(X^TX+\lambda I)^{-1}X^TXA$. En fait, si A correspond aux composantes principales classiques, alors on sait effectivement que B est proportionnelle à V (ce qu'on a montré dans la sous-section précédente).

Après avoir montré l'analogie entre les deux méthodes, on rajoute la pénalité de Lasso pour avoir la "sparsité" des coefficients de regression (ou de saturation). Le problème devient:

$$(\hat A, \hat B)= argmin_{A,B} \sum_{i=1}^n||x_i -AB^Tx_i||^2 +\lambda \sum_{j=1}^{k}||\beta_j||^2+ \sum_{j=1}^{k}\lambda_{1,j}||\beta_j||_1$$

Sous la contrainte:

$$A^TA=I_{k*k}$$

#### Outils pour appliquer la méthode

Voici les différents éléments qui seront nécéssaires pour appliquer la méthode et passer au travers de l'algorithme décrit dans la prochaine section.

1. On suppose que A est connu et on cherche B

$$\hat\beta_j= argmin_{\beta_j} ||Y_j -X\beta_j||^2 +\lambda||\beta_j||^2+ \lambda_{1,j}||\beta_j||_1$$

$$\hat\beta_j =arg min_{\beta_j}( \alpha_j-\beta_j)^TX^TX(\alpha_j-\beta_j)+\lambda||\beta_j||^2 +\lambda_1,j||\beta_j||_1$$

2. On suppose que B est connu et on cherche A (on ignore la pénalité qui est liée aux termes de $B$)

$$min_\beta \sum_{i=1}^k ||x_i-AB^Tx_i||^2 =||X-XBA^T||$$

Sous la contrainte:
$$A^TA=I_{k*k}$$

La solution est obtenu en utilisant le théorème 2, soit en calculant la décomposition en valeurs singulières (SVD) $(X^TX)B=UDV^T$ et posant $\hat A = UV^T$.

**Théorème 2**
Soit $M_{n\times p}$ et $N_{n\times k}$ deux matrices. On considère le problème de minimisation suivant
$$\hat A =argmin_A||M-NA^T||$$

Sous la contrainte
$$A^TA=I_{k*k}$$

La décomposition en valeurs singulières de $M^TN$ est $UDV^T$, donc $\hat A= UV^T$.

#### Algorithme pour appliquer la méthode

1. On commence par $A = [\alpha_1, \alpha_2, ..., \alpha_K]$ les coefficients de saturation des K premières composantes principales.

2. Sachant $A= [\alpha_{1}, ...,\alpha_{k}]$, on résout le problème
$$\beta_j =arg min_ \beta(\alpha_j-\beta)^TX^TX(\alpha_j-\beta)+\lambda||\beta||^2 +\lambda_1,j||\beta||_1$$
pour $j=1, 2, ..., k$. Cela nous permet d'obtenir une estimation de $B$.

3. Pour une matrice $B=[\beta_1, \beta_2, ..., \beta_k]$ fixée, on calcule la décomposition de la matrice en valeur singulière (SVD) de $X^TXB= UDV^T$. Par la suite on met A à jour, ce qui donne $A= UV^T$.

4. On répète les étapes 2-3 jusqu'à la convergence.

5. On normalise V, $\hat V_j = \frac{\beta_j}{||\beta_j||}$

Les $V_j$ sont les nouvelles composantes principales parcimonieuses.

## 4. Justification de la méthodologie

Pour justifier la méthodologie, il est possible de réaliser des simulations et voir comment performe l'analyse en composante principale parcimonieuse versus d'autres méthodes comme la méthode par rotation. Dans cet exemple, nous allons définir $3$ structures de vecteurs et valeurs propres différentes que nous cherchons à retrouver au travers de simulations. En effet, disons que nous avons une matrice et vecteurs propres $A$ et un vecteur de valeurs propres $I$, il est possible de retrouver la structure de corrélation de cette manière:

$$
\Sigma = A DA^{-1}
$$
où D est une matrice diagonale construite avec les valeurs propres($I$). De cette manière, on refait en quelque sorte le chemin inverse de la décomposition en valeurs et vecteurs propres de la structure de corrélation (ou covariance). Une fois que la structure de corrélation est retrouvée, il est possible de simuler des lois normales multivariées à partir de cette même structure.

Les 3 structures différentes de vecteurs et valeurs propres sont définis dans les tableaux 4, 5 et 6. La première est construite de manière à ce qu'il y aille 3 variables plus importantes et 3 variables moins importantes dans chacune des composantes. La troisième structure est construite de manière à avoir des coefficients de saturation proches les uns des autres pour chacune des composantes. Pour finir, la deuxième structure est une intermédiaire entre les deux structures décrites plus tôt.

La figure 1 présente les matrices de corrélation pour les données simulées à partir de ces $3$ différentes structures. Cela n'est pas si évident à voir, mais si on regarde de près la figure 1 on remarque que la structure en blocks contient des blocks de variables très corrélées (carré de couleur très uniforme), alors que la structure uniforme contient des variables avec des corrélations plus uniformément distribuées (carré de couleur moins uniforme).

```{r structure_blocks}
# Structure en blocks
vectors_blocks <- matrix(c(0.096, 0.082, 0.08, 0.594, 0.584, 0.533, -0.537, -0.565, -0.608, 0.085, 0.096, 0.074, 0.759, -0.599, -0.119, -0.074, -0.114, 0.18, -0.12, 0.231, -0.119, -0.308, -0.418, 0.805, 0.335, 0.511, -0.771, 0.069, 0.052, -0.157, -0.021, -0.013, 0.016, 0.731, -0.678, -0.069), nrow = 6, ncol = 6)
valeurs_blocks <- c(1.8367, 1.64, 0.751, 0.659, 0.607, 0.506)
data_structure_blocks <- data.table(
  variable = paste0("x", seq(1,6)),
  PC1 = vectors_blocks[,1],
  PC2 = vectors_blocks[,2],
  PC3 = vectors_blocks[,3],
  PC4 = vectors_blocks[,4],
  PC5 = vectors_blocks[,5],
  PC6 = vectors_blocks[,6]
)
knitr::kable(data_structure_blocks, format = "latex", booktabs = T, row.names = FALSE, align = rep("c", 6), caption = "Coefficients de saturation théoriques de la structure en blocks.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r structure_int}
# Structure intermediaire
vectors_intermediaires <- matrix(c(0.224, 0.253, 0.227, 0.553, 0.521, 0.507, -0.509, -0.519, -0.553, 0.249, 0.254, 0.199, 0.604, -0.361, -0.246, -0.249, -0.258, 0.561, 0.297, -0.644, 0.377, -0.052, 0.451, -0.384, -0.327, -0.341, 0.608, 0.262, -0.509, 0.281, 0.361, -0.064, -0.267, 0.706, -0.367, -0.402), nrow = 6, ncol = 6)
valeurs_intermediaires <- c(1.795, 1.674, 0.796, 0.618, 0.608, 0.510)
data_structure_intermediaire <- data.table(
  variable = paste0("x", seq(1,6)),
  PC1 = vectors_intermediaires[,1],
  PC2 = vectors_intermediaires[,2],
  PC3 = vectors_intermediaires[,3],
  PC4 = vectors_intermediaires[,4],
  PC5 = vectors_intermediaires[,5],
  PC6 = vectors_intermediaires[,6]
)
knitr::kable(data_structure_intermediaire, format = "latex", booktabs = T, row.names = FALSE, align = rep("c", 6), caption = "Coefficients de saturation théoriques de la structure intermédiaire.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r structure_platte}
# Structure platte
vectors_plattes <- matrix(c(-0.455, -0.439, -0.415, 0.434, 0.301, 0.385, 0.336, 0.370, 0.422, 0.458, 0.435, 0.416, -0.087, -0.212, 0.378, 0.040, -0.697, 0.563, 0.741, -0.630, -0.110, -0.136, 0.114, 0.104, -0.328, -0.445, 0.697, -0.167, 0.356, -0.234, 0.125, -0.175, 0.099, 0.744, -0.306, -0.545), nrow = 6, ncol = 6)
valeurs_plattes <- c(1.841, 1.709, 0.801, 0.649, 0.520, 0.480)
data_structure_plattes <- data.table(
  variable = paste0("x", seq(1,6)),
  PC1 = vectors_plattes[,1],
  PC2 = vectors_plattes[,2],
  PC3 = vectors_plattes[,3],
  PC4 = vectors_plattes[,4],
  PC5 = vectors_plattes[,5],
  PC6 = vectors_plattes[,6]
)
knitr::kable(data_structure_plattes, format = "latex", booktabs = T, row.names = FALSE, align = rep("c", 6), caption = "Coefficients de saturation théoriques de la structure uniforme.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r simulations}

# Load packages -----------------------------------------------------------

library(propagate)
library(psych)
library(elasticnet)

# Simuler les données -----------------------------------------------------

cov_function <- function(vecteurs_propres, valeurs_propres, cov = TRUE){
  
  n <- nrow(vecteurs_propres)
  S = vecteurs_propres
  M = matrix(rep(0, n^2), ncol = n, nrow = n)
  diag(M) <- valeurs_propres
  
  cor = round(S %*% M %*% solve(S), 6)
  
  for (j in 1:ncol(cor)) {
    for (i in 1:j) {
      if (cor[i, j] != diag(cor)[j]){
        cor[i, j] = cor[j, i]
      }
    }
  }
  
  if (cov){
    cov = round(propagate::cor2cov(C = cor, var = valeurs_propres), 6)
  } else {
    round(cor, 6)
  }
}

cov_blocks <- cov_function(vecteurs_propres = vectors_blocks, valeurs_propres = valeurs_blocks)
cov_intermediaire <- cov_function(vecteurs_propres = vectors_intermediaires, valeurs_propres = valeurs_intermediaires)
cov_platte <- cov_function(vecteurs_propres = vectors_plattes, valeurs_propres = valeurs_plattes)

data_blocks <- rmvnorm(100000, mean = rep(0, 6), sigma = cov_blocks)
data_intermediaire <- rmvnorm(100000, mean = rep(0, 6), sigma = cov_intermediaire)
data_platte <- rmvnorm(100000, mean = rep(0, 6), sigma = cov_platte)


# Appliquer les différentes méthodes --------------------------------------

acp_blocks <- prcomp(cor(data_blocks), center = FALSE)
rpca_blocks <- principal(cor(data_blocks), nfactors = 6, rotate = "varimax")
spca_blocks <- spca(cor(data_blocks), K = 6, para = rep(3,6), sparse = "varnum", type = "Gram")

acp_intermediaire <- prcomp(cor(data_intermediaire), center = FALSE)
rpca_intermediaire <- principal(cor(data_intermediaire), nfactors = 6, rotate = "varimax")
spca_intermediaire <- spca(cor(data_intermediaire), K = 6, para = rep(0.01, 6), sparse = "penalty", type = "Gram")

acp_platte <- prcomp(cor(data_platte), center = FALSE)
rpca_platte <- principal(cor(data_platte), nfactors = 6, rotate = "varimax")
spca_platte <- spca(cor(data_platte), K = 6, para = rep(0.01, 6), sparse = "penalty", type = "Gram")

# Analyser les performances -----------------------------------------------

dist_loadings <- function(x,y){
  sqrt(1 - (t(x) %*% y)^2)
}

# Structure blocks
# sapply(1:6, function(x) {
#   dist_loadings(acp_blocks$rotation[,x], vectors_blocks[,x])
# })
dist_rpca_blocks <- sapply(1:6, function(x) {
  dist_loadings(rpca_blocks$loadings[,x], vectors_blocks[,x])
})
dist_spca_blocks <- sapply(1:6, function(x) {
  dist_loadings(spca_blocks$loadings[,x], vectors_blocks[,x])
})

# Structure intermediaire
# sapply(1:6, function(x) {
#   dist_loadings(acp_intermediaire$rotation[,x], vectors_intermediaires[,x])
# })
dist_rpca_int <- sapply(1:6, function(x) {
  dist_loadings(rpca_intermediaire$loadings[,x], vectors_intermediaires[,x])
})
dist_spca_int <- sapply(1:6, function(x) {
  dist_loadings(spca_intermediaire$loadings[,x], vectors_intermediaires[,x])
})

# Structure platte
# sapply(1:6, function(x) {
#   dist_loadings(acp_plattes$rotation[,x], vectors_plattes[,x])
# })
dist_rpca_platte <- sapply(1:6, function(x) {
  dist_loadings(rpca_platte$loadings[,x], vectors_plattes[,x])
})
dist_spca_platte <- sapply(1:6, function(x) {
  dist_loadings(spca_platte$loadings[,x], vectors_plattes[,x])
})
```

```{r cor_plot, echo = FALSE, fig.align = 'center', fig.cap = "Représentations des différentes matrices de corrélation pour les données simulées à partir des 3 différentes structures."}
library(reshape2)
library(ggplot2)
library(gridExtra)
library(dplyr)

colnames(data_blocks) <- paste0("Var", seq(1:6))
colnames(data_intermediaire) <- paste0("Var", seq(1:6))
colnames(data_platte) <- paste0("Var", seq(1:6))

cor_blocks <- round(cor(data_blocks), 2)
cor_intermediaires <- round(cor(data_intermediaire), 2)
cor_platte <- round(cor(data_platte), 2)

melted_cor_blocks <- data.table(melt(cor_blocks))[, structure := "Block"]
melted_cor_int <- data.table(melt(cor_intermediaires))[, structure := "Intermédiaire"]
melted_cor_platte <- data.table(melt(cor_platte))[, structure := "Uniforme"]

rbindlist(list(melted_cor_blocks, melted_cor_int, melted_cor_platte), use.names = TRUE, fill = FALSE) %>% 
  ggplot(aes(x = Var1, y = Var2, fill = value)) + 
    geom_tile() +
    facet_grid(. ~ structure) +
    scale_fill_continuous("Corrélation") +
    theme(legend.position = "bottom", axis.title.x = element_blank(), axis.title.y = element_blank())
```

À partir des données simulées, il est possible d'appliquer l'ACP suivi de la rotation varimax ainsi que l'ACP parcimonieuse et voir quelle des deux méthodes permet le mieux de retrouver les structures définit initialement. Les tableaux 7, 8 et 9 montrent les distances entre les vecteurs de coefficients de saturation trouvés par les deux méthodes avec les "vrais" vecteurs de coefficients définit dans les 3 structures différentes. 

Dans les tableaux 7, 8 et 9, on remarque que la méthode d'analyse en composante principale parcimonieuse (SPCA) retrouve des coefficients de corrélation qui sont plus près de ceux définis au départ, et ce, pour les $3$ types de structure.

```{r distance_blocks}
# Structure en blocks
data_structure_blocks <- data.table(
  methode = c("RPCA", "SPCA"),
  PC1 = c(dist_rpca_blocks[1], dist_spca_blocks[1]),
  PC2 = c(dist_rpca_blocks[2], dist_spca_blocks[2]),
  PC3 = c(dist_rpca_blocks[3], dist_spca_blocks[3]),
  PC4 = c(dist_rpca_blocks[4], dist_spca_blocks[4]),
  PC5 = c(dist_rpca_blocks[5], dist_spca_blocks[5]),
  PC6 = c(dist_rpca_blocks[6], dist_spca_blocks[6])
)
knitr::kable(data_structure_blocks, format = "latex", booktabs = T, row.names = FALSE, col.names = c("Méthode", paste0("PC", seq(1,6))), align = rep("c", 7), caption = "Distances entre les vecteurs de coefficients de saturation reconstruits d'après les 2 méthodes avec les vecteurs définis dans la structure en blocks.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r distance_int}
# Structure intermediaire
data_structure_intermediaire <- data.table(
  methode = c("RPCA", "SPCA"),
  PC1 = c(dist_rpca_int[1], dist_spca_int[1]),
  PC2 = c(dist_rpca_int[2], dist_spca_int[2]),
  PC3 = c(dist_rpca_int[3], dist_spca_int[3]),
  PC4 = c(dist_rpca_int[4], dist_spca_int[4]),
  PC5 = c(dist_rpca_int[5], dist_spca_int[5]),
  PC6 = c(dist_rpca_int[6], dist_spca_int[6])
)
knitr::kable(data_structure_intermediaire, format = "latex", booktabs = T, row.names = FALSE, col.names = c("Méthode", paste0("PC", seq(1,6))), align = rep("c", 7), caption = "Distances entre les vecteurs de coefficients de saturation reconstruits d'après les 2 méthodes avec les vecteurs définis dans la structure en intermédiaire.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

```{r distance_platte}
# Structure platte
data_structure_platte <- data.table(
  methode = c("RPCA", "SPCA"),
  PC1 = c(dist_rpca_platte[1], dist_spca_platte[1]),
  PC2 = c(dist_rpca_platte[2], dist_spca_platte[2]),
  PC3 = c(dist_rpca_platte[3], dist_spca_platte[3]),
  PC4 = c(dist_rpca_platte[4], dist_spca_platte[4]),
  PC5 = c(dist_rpca_platte[5], dist_spca_platte[5]),
  PC6 = c(dist_rpca_platte[6], dist_spca_platte[6])
)
knitr::kable(data_structure_platte, format = "latex", booktabs = T, row.names = FALSE, col.names = c("Méthode", paste0("PC", seq(1,6))), align = rep("c", 7), caption = "Distances entre les vecteurs de coefficients de saturation reconstruits d'après les 2 méthodes avec les vecteurs définis dans la structure uniforme.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

## 5. Application de la méthodologie

Cette partie a comme but de revenir au jeu de données présenté dans la section "Exemple de motivation" et voir comment se comporte les méthodes introduites dans ce document. On se rappelle que l'ACP classique appliquée sur le jeu de données pitprops permettait de définir des composantes principales qui étaient difficiles à interpréter. Pour palier à ce problème, il était possible de faire une rotation de ces composantes. Toutefois, cela amenait d'autres problèmes comme la perte d'information dans les premières composantes principales ainsi que d'autres problèmes d'interprétabilité.

Dans cette section, nous allons donc comparer les résultats obtenus par les trois méthodes suivantes:

1. L'ACP classique (PCA)
2. L'ACP suivi d'une rotation varimax (RPCA)
3. L'analyse en composante principale parcimonieuse (SPCA)

La première méthode (ACP) est implémentée dans plusieurs librairies *R*, mais pour l'exemple nous avions utilisé la fonction *prcomp* de la librairie **stats**. Pour la deuxième méthode (RPCA), nous avons utilisé la fonction *principal* de la librairie **psych**. Pour la dernière méthode, nous avons utilisé la librairie **elasticnet** et la fonction *spca*.

Les hyperparamètres de la méthode SPCA ont été définis dans le but d'obtenir une variance expliquée similaire à celle de l'ACP pour les $6$ premières composantes principales. Voici les paramètres:

\begin{gather*}
  \lambda=0.000001 \\
  \lambda_1= [0.06, 0.16, 0.1, 0.5, 0.5, 0.5]
\end{gather*}

On commence par définir les hyperparamètres définits ci-dessus et on applique la fonction sur le jeu de données pitpros.

```{r spca, echo = TRUE}
library(elasticnet)
data("pitprops")

lambda <- 0.000001
lambda_1 <- c(0.06, 0.16, 0.1, 0.5, 0.5, 0.5)

spca_pitprops <- spca(pitprops, K = 6, type = "Gram", sparse = "penalty",
                      para = lambda_1, lambda = lambda)
```

En appliquant la méthode SPCA sur le jeu de données, on obtient des coefficients de saturation qui sont en fonction d'un plus petit nombre de variables. Ces différents vecteurs de coefficients de saturation sont présentés dans le tableau 4. 

```{r application}
data_spca <- data.table(
  variable = c(paste0("x", seq(1:ncol(pitprops))), "Variance (%)", "Cumulative variance (%)"),
  PC1 = round(c(as.numeric(spca_pitprops$loadings[, 1]), spca_pitprops$pev[1], sum(spca_pitprops$pev[1:1])), 3),
  PC2 = round(c(as.numeric(spca_pitprops$loadings[, 2]), spca_pitprops$pev[2], sum(spca_pitprops$pev[1:2])), 3),
  PC3 = round(c(as.numeric(spca_pitprops$loadings[, 3]), spca_pitprops$pev[3], sum(spca_pitprops$pev[1:3])), 3),
  PC4 = round(c(as.numeric(spca_pitprops$loadings[, 4]), spca_pitprops$pev[4], sum(spca_pitprops$pev[1:4])), 3),
  PC5 = round(c(as.numeric(spca_pitprops$loadings[, 5]), spca_pitprops$pev[5], sum(spca_pitprops$pev[1:5])), 3),
  PC6 = round(c(as.numeric(spca_pitprops$loadings[, 6]), spca_pitprops$pev[6], sum(spca_pitprops$pev[1:6])), 3)
)

knitr::kable(data_spca, format = "latex", booktabs = T, row.names = FALSE, col.names = c("", paste0("PC", seq(1,6))), align = rep("c", 6), caption = "Coefficients de saturation de l'analyse en composante principale parcimonieuse (SPCA) effectuée sur la matrice de corrélation du jeu de données pitprops.") %>% 
  kable_styling(latex_options = c("striped", "hold_position"))
```

En analysant le tableau 10, on remarque que 7 variables sur 13 sont non nulles pour la première composante principale. Pour la deuxième et la troisième, 4 sur 13 sont non nulles. Cela permet donc de faciliter l'interprétation des composantes, car on peut immédiatement écarter les coefficients nuls, comparativement à la méthode de la rotation, qui ne permettait pas cela. Ainsi, on peut conclure que la méthode d'analyse en composante principale parcimonieuse donne les résultats les plus faciles à interpréter, suivi dans l'ordre par la méthode de la rotation et de l'ACP classique. Il est également possible de voir, pour chacune des composantes principales, si le nombre de coefficients nuls de la méthode SPCA sont enlignés avec le nombre coefficients "faibles" (<= 0.2) des autres méthodes.

```{r graph_coeff, fig.align='center', fig.cap="Nombre de coefficients de saturation nuls ou faible (<=0.2) selon les différentes méthodes."}
acp_rotated <- principal(pitprops, 6, rotate = "varimax", eps = 1e-14)
acp <- prcomp(x = pitprops, center = FALSE)

data.table(
  PC = rep(as.character(seq(1:6)), 3),
  methode = c(rep("SPCA", 6), rep("RPCA", 6), rep("PCA", 6)),
  nb_nuls = c(sapply(1:6, function(x) sum(spca_pitprops$loadings[,x] == 0)), sapply(1:6, function(x) sum(abs(acp_rotated$loadings[,x]) <= 0.3)), sapply(1:6, function(x) sum(abs(acp$rotation[,x]) <= 0.3)))
) %>% 
  ggplot(aes(x = PC, y = nb_nuls, group = methode, color = methode)) +
    geom_point() +
    geom_line() +
    scale_x_discrete("Composante principale") +
    scale_y_continuous("Nombre de coefficents nuls ou plus petit que 0.2") +
    scale_color_discrete("Méthode") +
    theme_classic()
```

On remarque à la figure 2 que la régularisation faite par la méthode SPCA est forte sur les composantes 4 à 6. Cela est effectivement contrôlé par l'hyperparamètre $\lambda_1$ définit un peu plus tôt. Pour les premières composantes, on voit qu'il y a un lien entre le nombres de coefficients nuls de la méthode SPCA et le nombre de coefficients faibles des autres méthodes. Cela veut donc dire que les coefficients qui ont été tiré vers $0$ sont probablement des coefficients "peu importants" et sont ceux que nous aurions exclut intuitivement de notre interprétation.

Comme mentionné précédemment, l'amélioration de l'interprétabilité se fait souvent au prix de perte de l'information, ou de la variabilité dans les composantes principales. En regardant de plus près les tableaux 2, 3 et 4, on remarque que la rotation varimax et la méthode SPCA ont le même genre de pertes au niveau de la variabilité. Toutefois, étant donné que nous avons un gain au niveau de l'interprétabilité avec la méthode d'analyse en composante principale parcimonieuse, il est possible de conclure que celle-ci constitue une amélioration par rapport aux autres méthodes comme la rotation.

## 6. Autres considérations de la méthode

Dans ce document, nous avons décrits deux méthodes en lien avec l'analyse en composantes principales parcimonieuse, soit la méthode SCoTLASS et la méthode SPCA. Il est pertinent de mentionner qu'une autre méthode a été développée, soit la méthode PMD [@Witten]. Cette dernière est basée sur une décomposition pénalisée de la matrice de covariance (ou corrélation). Il existe d'ailleurs une implémentation de cette méthode en **R** qu'on peut retrouver dans la librairie *PMA*.

## 7. Annexe

### Les méthodes de régularisation (Ridge, Lasso et Elastic net)

Dans le domaine des mathématiques et de la statistique, plus particulièrement dans le domaine de l'apprentissage automatique, la régularisation fait référence à un processus consistant à ajouter de l'information à un problème pour éviter le **surapprentissage**. Cette information prend généralement la forme d'une pénalité.

Une méthode généralement utilisée est de pénaliser les valeurs extrêmes des paramètres, qui sont souvent associées à un surapprentissage. Pour cela, on va utiliser une norme sur ces paramètres que l'on va ajouter à la fonction qu'on cherche à minimiser. Les normes les plus couramment employées pour cela sont les normes $L_1$ et $L_2$. La norme $L_1$ offre l'avantage de faire une sélection de paramètres, mais elle n'est pas différentiable, ce qui peut être un inconvénient pour les algorithmes utilisant un calcul de gradient pour l'optimisation. Cette régularisation permet alors d'avoir des coefficients qui peuvent être estimés exactement à zéro. Par conséquent, ces méthodes peuvent effectuer une sélection des variables importantes pour la variable réponse.

Dans le cadre d'une régression multiple $Y= X\beta +\epsilon$ tel que $\beta=(\beta_0, \beta_1,\beta_2, ..., \beta_p)^T$ et $X=(1,X_1,...,X_p)$, on se trouve parfois dans la situation où le nombre de paramètres ($p$) est supérieur au nombre d'observations ($n$). En présence de ce nombre élevé de prédicteurs, il sera nécessaire de réduire le nombre de paramètres, car la solution donnée par la méthode des moindres carrés classique ne sera pas unique et la variance aura tendance à être élevée avec un petit biais. Ainsi, les méthodes de régularisation offrirons une réduction importante de variance et une petite augmentation du biais. 


#### La méthode de Ridge

La méthode de Ridge est une technique de régularisation qui se base sur la norme $L_2$. Cette méthode a comme pénalité $p(\beta)= ||\beta||^2_2$ avec $||\beta||^2_2 = \sum_{i=1}^{p} \beta^2_i$.

L'estimateur de Ridge de $\beta$ est défini par

$$\hat \beta^{Ridge}= argmin_\beta \{\sum_{i=1}^n(y_i-\beta_0 -\sum_{j=1}^p x_{ij}\beta_j)^2\}$$
sous la contrainte de $\sum_{j=1}^p \beta^2_j \leq t$.

L'estimateur de Ridge est donc donné par $\hat\beta^{Ridge}=(X^TX+\lambda I)^{-1}X^T y$.

Cette méthode est utilisée dans le cas où il y a une corrélation entre les variables explicatives. Dans ce cas-ci, $X^TX$ a des valeurs proche de zéro et la MMCO n'est pas satisfaisante. Cette méthode permet donc d'ajouter un terme $\lambda$ pour augmenter la valeur de $X^TX$ et ainsi les rendre plus stables. De ce fait, la méthode contrôle donc la variance des estimateurs en pénalisant les grandes valeurs de $\hat\beta$ ce qui a comme avantage l'obtention d'une erreur de prédiction moins faible. Par contre, elle ne permet pas d'avoir un modèle parcimonieux, car on ne pénalise pas les variables nuisibles par des coefficients nuls. Au final, on introduit toutes les variables explicatives dans le modèle ce qui peut compliquer l'interprétation du modèle. 

#### La méthode du Lasso

Cette méthode est basée sur la norme $L_1$. La pénalité de cette méthode est quant à elle donnée par $p(\beta)=||\beta||_1$ avec $||\beta||_1 = \sum_{i=1}^{p} |\beta_i|$.

L'estimateur de $\beta$ par la méthode du Lasso est défini par

$$\hat\beta^{Lasso}= argmin_\beta \{\sum_{i=1}^n(y_i-\beta_0 -\sum_{j=1}^p x_{ij}\beta_j)^2\}$$

avec $\sum_{j=1}^p |\beta|_j \leq t$.

Par un calcul analytique, la solution de ce problème est donnée par

$$\hat\beta^{lasso}_j=signe\{\hat\beta^{MCO}_j\}(|\hat\beta^{MCO}_j|-\lambda)_+$$

Cette méthode permet de créer une parcimonie, car elle élimine les variables nuisibles dans le modèle en estimant leur coefficients par des zéros. Cela permet de réduire le nombre de coefficients ($\beta$) en conservant les variables qui contribuent le plus dans le modèle. Par contre, elle est inappropriée dans le cas où un ensemble de prédicteurs est fortement corrélé, car elle choisit un parmi ceux-ci et annule les autres. Sans oublié le fait qu'elle choisit un maximum de $n$ variables dans le cas où $p > n$. Ces problèmes sont traités par la méthode qui suit.

#### La méthode Elastic Net

Cette méthode de régularisation combine les deux normes ($L_1$ et $L_2$). On peut donc la voir comme un compromis entre la méthode du Lasso et la méthode de Ridge. Sa pénalité est donnée par $p(\beta)=\lambda_1 ||\beta||_1 +\lambda_2 ||\beta||^2_2$.

L'estimateur de $\beta$ par la méthode de l'Elastic Net est défini par

$$\hat\beta^{EN}= argmin_\beta\{||y-X\beta||^2\ + \lambda_1 ||\beta||_1 +\lambda_2 ||\beta||^2_2 \}$$

En introduisant la pénalité $L_1$, on ajoute la possibilité d'obtenir un modèle parcimonieux. L'ajout de la pénalité quadratique $L_2$ assume la suppression de la limitation du nombre des variables sélectionnées, encourage l'effet du groupe (group effect), car elle permet de sélectionner un nombre de paramètres ($p$) supérieur à $n$. De plus, elle permet de tenir en compte la corrélation entre les prédicteurs.

## 8. Bibliographie
